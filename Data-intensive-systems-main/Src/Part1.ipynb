{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44673590-17e6-4c2b-ac9e-c5b759971dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import concat,concat_ws, lit, length,col, expr, count , row_number,collect_list,struct,udf,array,array_union, explode\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import time\n",
    "import findspark\n",
    "import csv\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dab37d-1d4a-4b16-9a44-571c84826386",
   "metadata": {},
   "source": [
    "# PART 1:\n",
    "## 1. Grouping the similar processes according to Jaccard Similarities\n",
    "## 2. Creating the new data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824c81f-680a-419a-93a9-fc8274862528",
   "metadata": {},
   "source": [
    "# code to start the Master:\n",
    "1. Open cmd and admin\n",
    "2. write \"cd %SPARK_HOME%\"\n",
    "3. bin\\spark-class2.cmd org.apache.spark.deploy.master.Master\n",
    "# code to start the worker:\n",
    "1. Open cmd and admin\n",
    "2. write \"cd %SPARK_HOME%\"\n",
    "3. write \"bin\\spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 2 -m 6G spark://192.168.1.81:7077\"\n",
    "* in step 3:\n",
    "* -c -> number of cores\n",
    "* -m -> amount of RAM for the current worker\n",
    "* the spark link is from the Master link ( go to the web page of the master and locate the spark link )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc13e44-35b7-4173-8001-0d1ef82635e0",
   "metadata": {},
   "source": [
    "# Initializing the Spark Aplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "443c6a1c-69c9-490b-8c70-40ed9281d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"part1Grouping\") \\\n",
    "    .master(\"spark://192.168.1.81:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"3600s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251ac2c-81b6-4073-a6bc-919585a06cc3",
   "metadata": {},
   "source": [
    "# Converting Input File to CSV Format for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "454949a9-1e7c-4598-8187-828e074a68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "Skipping malformed line: \n",
      "['']\n",
      "Skipping malformed line: \n",
      "['']\n",
      "Skipping malformed line: \n",
      "['']\n",
      "Skipping malformed line: \n",
      "['']\n",
      "Skipping malformed line: \n",
      "['']\n",
      "Skipping malformed line: \n",
      "['']\n",
      "Skipping malformed line: \n",
      "Data has been successfully written to reverse_cases.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = 'reverse_cases.txt'\n",
    "output_file = 'reverse_cases.csv'\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    " \n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip().strip('<>')\n",
    "    parts = line.split(',')\n",
    "    if len(parts) != 5:\n",
    "        print (parts)\n",
    "        print(f\"Skipping malformed line: {line}\")\n",
    "        continue\n",
    "    try:\n",
    "        processed_line = {\n",
    "            'FromServer': parts[0].strip(\"'\"),\n",
    "            'ToServer': parts[1].strip(),\n",
    "            'time': int(parts[2].strip()),\n",
    "            'action': parts[3].strip(),\n",
    "            'processId': int(parts[4].strip())\n",
    "        }\n",
    "        processed_lines.append(processed_line)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing line: {line}. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# exporting to CSV\n",
    "headers = ['FromServer', 'ToServer', 'time', 'action', 'processId']\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for row in processed_lines:\n",
    "        writer.writerow(row)\n",
    " \n",
    "print(f\"Data has been successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef503522-962d-4440-89cb-1e74026e26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "data_path = \"reverse_cases.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3d27b1d-6b56-4f22-8885-67af12f535ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group by processID and collect the sequence of actions\n",
    "processes_df = df.groupBy(\"processID\").agg(collect_list(struct(\"FromServer\", \"ToServer\", \"time\", \"action\")).alias(\"actions\"))\n",
    "\n",
    "# Convert actions to string for MinHash LSH\n",
    "def actions_to_string(actions):\n",
    "    return \"\".join([f\"{action['FromServer']}{action['ToServer']}\" for action in actions if action['action'] != 'Response'])\n",
    "\n",
    "\n",
    "\n",
    "actions_to_string_udf = udf(actions_to_string, StringType())\n",
    "processes_df = processes_df.withColumn(\"actions_str\", actions_to_string_udf(col(\"actions\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c203e-f063-4c51-b8a4-e6ac7df9ebc5",
   "metadata": {},
   "source": [
    "# Calculating the K for our Shingles, based on the median length of our processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cda6f26-141d-4d3b-b431-73b048182c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the median  Row(length=44)  we chose  5 -shingles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_with_length = processes_df.withColumn(\"length\", length(\"actions_str\"))\n",
    "windowSpec = Window.orderBy(\"length\")\n",
    "df_with_length = df_with_length.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "count_df = df_with_length.count()\n",
    "\n",
    "median_row = math.ceil(count_df / 2.0)\n",
    "\n",
    "median_length = df_with_length.filter(col(\"row_num\") == median_row).select(\"length\").first()\n",
    "\n",
    "cur_k = 5\n",
    "thresholds = [(10000, 9), (5000, 8), (1000, 7), (100, 6)]\n",
    "for threshold, value in thresholds:\n",
    "    if median_length[0] > threshold:\n",
    "        cur_k = value\n",
    "        break\n",
    "print(\"for the median \",median_length,\" we chose \",cur_k,\"-shingles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0e502-ff63-40a9-8498-37b4dd9cc0f6",
   "metadata": {},
   "source": [
    "# Computing the K-Shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8079d96a-2044-4c38-a001-4c14c5c4af0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert actions string into shingles\n",
    "def get_shingles(row, k=5):\n",
    "    concatenated_str = ''.join(row)\n",
    "    shingles = [concatenated_str[i:i+k] for i in range(len(concatenated_str) - (k - 1))]\n",
    "    return shingles\n",
    "\n",
    "\n",
    "get_shingles_udf = udf(lambda x: get_shingles(x,cur_k), ArrayType(StringType()))\n",
    "processes_df = processes_df.withColumn(\"shingles\", get_shingles_udf(col(\"actions_str\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a097b3c4-b0b0-40c8-8190-2f43a2d3511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time that it takes our model to find the pairs is  5.434313058853149 with number of pairs =  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\",binary=True)\n",
    "cv_model = cv.fit(processes_df)\n",
    "vectorized_df = cv_model.transform(processes_df)\n",
    "start = time.time()\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "mh_model = mh.fit(vectorized_df)\n",
    "\n",
    "hashed_df = mh_model.transform(vectorized_df)\n",
    "threshold = 0.5\n",
    "# Find similar candidate process IDs using MinHashLSH\n",
    "similarity_df = mh_model.approxSimilarityJoin(hashed_df, hashed_df, threshold, distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processID\").alias(\"processID_A\"),\n",
    "            col(\"datasetB.processID\").alias(\"processID_B\"),\n",
    "            col(\"JaccardDistance\"),col(\"datasetA.features\").alias(\"featuresA\"),col(\"datasetB.features\").alias(\"featuresB\"))\n",
    "\n",
    "# Filter out self-joins and duplicates\n",
    "similarity_df = similarity_df.filter(col(\"processID_A\") < col(\"processID_B\"))\n",
    "\n",
    "# Function to calculate Jaccard similarity\n",
    "def jaccard_similarity(vec1, vec2):\n",
    "    set1 = set(vec1.indices)\n",
    "    set2 = set(vec2.indices)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "    return float(len(intersection)) / len(union)\n",
    "\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "# Calculate Jaccard similarity for each candidate pair\n",
    "similarity_df = similarity_df.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"featuresA\"), col(\"featuresB\")))\n",
    "# Filter pairs with Jaccard similarity above a threshold (e.g., 90%)\n",
    "similarity_df_filtered = similarity_df.filter(col(\"JaccardSimilarity\") >= 0.9)\n",
    "\n",
    "#grouping the similar pair processes:\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"group_representative\"))\n",
    "sum = similarity_df_filtered.count()\n",
    "end = time.time()\n",
    "print(\"the time that it takes our model to find the pairs is \",end-start, \"with number of pairs = \",sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bdec6-56d8-4c47-82d3-c59b84223de8",
   "metadata": {},
   "source": [
    "# Calculating the time for the baseline model\n",
    "### Comparing all the possible pairs without using minHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "193acd3c-42f9-4f99-a874-cd545f573a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# original_eval = vectorized_df.alias(\"df1\").join(vectorized_df.alias(\"df2\")).select(col(\"df1.processId\").alias(\"processIdA\"),col(\"df1.features\").alias(\"processAFeatures\"),\n",
    "#                                                                   col(\"df2.processId\").alias(\"processIdB\"),col(\"df2.features\").alias(\"processBFeatures\")).orderBy(col(\"processIdA\"))\n",
    "# original_eval = original_eval.filter(col(\"processIdA\") < col(\"processIdB\"))\n",
    "# original_eval = original_eval.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"processAFeatures\"), col(\"processBFeatures\")))\n",
    "# original_eval = original_eval.filter(col(\"JaccardSimilarity\") >= 0.9)\n",
    "# sum_original = original_eval.count()\n",
    "# end = time.time()\n",
    "# print(\"the time that it takes our model to find the pairs is \",end-start, \"with number of pairs = \",sum_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b65b8-5b9f-43d1-83eb-abebf4abc95c",
   "metadata": {},
   "source": [
    "# Applying Transitivity to Grouped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5dcfb7e-4d37-4ddc-a840-b731a251c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of groups  0\n"
     ]
    }
   ],
   "source": [
    "# Merge overlapping groups\n",
    "def merge_groups(group_list):\n",
    "    groups = []\n",
    "    for group in group_list:\n",
    "        merged = False\n",
    "        for existing_group in groups:\n",
    "            if any(item in group for item in existing_group):\n",
    "                existing_group.update(group)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            groups.append(set(group))\n",
    "    return [list(group) for group in groups]\n",
    "\n",
    "merge_groups_udf = udf(lambda x: merge_groups(x), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "grouped_lists = exploded_df.groupBy(\"group_representative\") \\\n",
    "    .agg(collect_list(\"processID\").alias(\"group_list\")) \\\n",
    "    .agg(collect_list(\"group_list\").alias(\"group_lists\"))\n",
    "\n",
    "merged_groups = grouped_lists.withColumn(\"merged_groups\", merge_groups_udf(col(\"group_lists\"))) \\\n",
    "    .select(explode(col(\"merged_groups\")).alias(\"final_group\"))\n",
    "\n",
    "final_groups_df = merged_groups.select(concat_ws(\"_\", col(\"final_group\")).alias(\"Group\"), col(\"final_group\"))\n",
    "\n",
    "print(\"the number of groups \", final_groups_df.count())\n",
    "\n",
    "final_groups_exploded = final_groups_df.withColumn(\"processID\", explode(col(\"final_group\")))\n",
    "\n",
    "filtered_df = df.join(final_groups_exploded, on=\"processID\", how=\"inner\")\n",
    "\n",
    "group_representative_df = final_groups_exploded.groupBy(\"Group\").agg(min(\"processID\").alias(\"representative_processID\"))\n",
    "\n",
    "representative_processes_df = group_representative_df.join(filtered_df, filtered_df[\"processID\"] == group_representative_df.representative_processID, \"inner\") \\\n",
    "    .select(\"processID\", \"FromServer\", \"ToServer\", \"time\", \"action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095810c5-3cf9-4388-babd-138c4570ec82",
   "metadata": {},
   "source": [
    "# Creating the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19b5802c-cd0d-4286-b8fb-2eebdb8403b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes_to_remove = final_groups_df.selectExpr(\"explode(final_group) as processID\").distinct()\n",
    "\n",
    "df_without_groups = df.join(processes_to_remove, \"processID\", \"left_anti\")\n",
    "\n",
    "df_without_groups = df_without_groups.select(\"FromServer\", \"ToServer\", \"time\", \"action\",\"processID\")\n",
    "\n",
    "constant_number = df.agg({\"processID\": \"max\"}).first()[0]\n",
    "new_representative_processes_df = representative_processes_df.withColumn(\n",
    "    \"processID\",\n",
    "    expr(f\"processID + {constant_number}\"))\n",
    "\n",
    "new_representative_processes_df = new_representative_processes_df.select(\"FromServer\", \"ToServer\", \"time\", \"action\",\"processID\").orderBy(\"time\")\n",
    "\n",
    "# Combine original DataFrame and representatives DataFrame\n",
    "combined_df = df_without_groups.union(new_representative_processes_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fd6b4-fc29-4be1-bfe0-1fca39b52f08",
   "metadata": {},
   "source": [
    "# creating the txt files:\n",
    "## The desired files will be in the folder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fedf60b6-3e4f-4f7f-98ee-0a6a078a2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_one_txt(df, local_path_name,wanted_list):\n",
    "    correct_path = wanted_list + \"/part1Output.txt\"\n",
    "    formatted_df = df.withColumn(\n",
    "    \"formatted_line\",\n",
    "    concat(lit(\"<\"), df.FromServer, lit(\",\"),\n",
    "           df.ToServer, lit(\",\"),\n",
    "           df.time, lit(\",\"),\n",
    "           df.action, lit(\",\"),\n",
    "           df.processID, lit(\">\"))\n",
    ")\n",
    "    open(correct_path, \"w\")\n",
    "    formatted_df.select(\"formatted_line\").write.mode(\"overwrite\").text(output_path)\n",
    "    os.system(f'cat {local_path_name}/*.txt >> {correct_path}')\n",
    "    os.system(f'rm -r {local_path_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6d9f5fb-c15b-4912-9cbe-951cb816d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./part1OUT1\"\n",
    "output_path1 = \"./output\"\n",
    "write_to_one_txt(combined_df,output_path,output_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5eb262e7-90dd-43b2-a7c0-c429b39d384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+\n",
      "|processId|FromServer|ToServer|time|action|\n",
      "+---------+----------+--------+----+------+\n",
      "+---------+----------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a dataframe only with the processes that were grouped.\n",
    "df_with_groups = df.join(processes_to_remove, \"processID\", \"semi\")\n",
    "df_with_groups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f0ff333-dfeb-45fb-81fa-ae4feb700f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_final_groups_df = final_groups_df.select(\"Group\", explode(\"final_group\").alias(\"processID\"))\n",
    "joined_df = df_with_groups.join(exploded_final_groups_df, \"processID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "023aabea-fe0c-472b-8408-b625b24103f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write groups to txt file\n",
    "def write_groups_to_txt(grouped_df, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        for row in grouped_df.collect():\n",
    "            group_name = row[\"Group\"]\n",
    "            process_ids = row[\"processIDs\"]\n",
    "            formatted_rows = row[\"formatted_rows\"]\n",
    "            \n",
    "            # Ensure process_ids are unique and sorted\n",
    "            process_ids = sorted(set(process_ids))\n",
    "            \n",
    "            file.write(f\"Group: {{{', '.join(map(str, process_ids))}}}\\n\")\n",
    "            \n",
    "            for process_id in process_ids:\n",
    "                file.write(f\"{process_id}:\\n\")\n",
    "                \n",
    "                # Find all formatted rows for the current process ID\n",
    "                rows_for_process_id = [row for row in formatted_rows if row.endswith(f\",{process_id}>\")]\n",
    "                \n",
    "                if rows_for_process_id:\n",
    "                    for formatted_row in rows_for_process_id:\n",
    "                        file.write(f\"{formatted_row}\\n\")\n",
    "                else:\n",
    "                    file.write(\"<No corresponding formatted rows found>\\n\")\n",
    "                    \n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "481a4565-628c-4fe5-9798-795445364f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = joined_df.withColumn(\n",
    "    \"formatted_row\",\n",
    "    concat_ws(\"\", lit(\"<\"), col(\"FromServer\"), lit(\",\"), col(\"ToServer\"),\n",
    "              lit(\",\"), col(\"time\"), lit(\",\"), col(\"action\"), lit(\",\"), col(\"processID\"), lit(\">\"))\n",
    ")\n",
    "grouped_df = formatted_df.groupBy(\"Group\").agg(\n",
    "    collect_list(\"processID\").alias(\"processIDs\"),\n",
    "    collect_list(\"formatted_row\").alias(\"formatted_rows\")\n",
    ")\n",
    "# Output path\n",
    "output_path = \"./output/part1Observations.txt\"\n",
    "\n",
    "write_groups_to_txt(grouped_df, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1de14962-0794-44a4-bd78-86cf7728c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8558256-42d6-4d55-86f7-93af20561192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b744a8e-6d2f-4767-9344-d03214e01ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057e3df-c6a5-457e-9fca-54fdfeb6b746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
