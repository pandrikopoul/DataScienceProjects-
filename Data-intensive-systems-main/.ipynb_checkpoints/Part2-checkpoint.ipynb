{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bd36e2e-31b2-4226-9c1b-6ad0058fbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import concat,concat_ws, lit,col, count,udf,collect_list,array, array_union, explode,collect_set\n",
    "from pyspark.sql.window import Window\n",
    "import findspark\n",
    "import csv\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890733c3-c79d-4f7e-924d-577e729a2ec4",
   "metadata": {},
   "source": [
    "# PART 2:\n",
    "## 1. Grouping the similar processes according to Jaccard Similarities\n",
    "## 2. Creating the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbf018cb-bfbb-4093-9672-f54ec3b85417",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"part2Grouping\") \\\n",
    "    .master(\"spark://192.168.1.81:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"3600s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "809f1609-3c15-4ce6-9636-623d930388a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to part1Output.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = 'part1Output.txt'\n",
    "output_file = 'part1Output.csv'\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    " \n",
    "# Preprocess the lines to handle custom format\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip().strip('<>')\n",
    "    parts = line.split(',')\n",
    "    if len(parts) != 5:\n",
    "        print(f\"Skipping malformed line: {line}\")\n",
    "        continue\n",
    "    try:\n",
    "        processed_line = {\n",
    "            'FromServer': parts[0].strip(\"'\"),\n",
    "            'ToServer': parts[1].strip(),\n",
    "            'time': int(parts[2].strip()),\n",
    "            'action': parts[3].strip(),\n",
    "            'processId': int(parts[4].strip())\n",
    "        }\n",
    "        processed_lines.append(processed_line)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing line: {line}. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# exporting to CSV\n",
    "headers = ['FromServer', 'ToServer', 'time', 'action', 'processId']\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for row in processed_lines:\n",
    "        writer.writerow(row)\n",
    " \n",
    "print(f\"Data has been successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6a772d6-5e90-49a7-bc17-c892f8b7ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"reverse_cases.csv\"\n",
    "dataForPart2 = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Aggregate FromServer and ToServer into sets for each processId\n",
    "agg_df = dataForPart2.groupBy(\"processId\").agg(\n",
    "    collect_set(\"FromServer\").alias(\"servers_array\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c252b5dd-737a-4739-842e-a6eea2960dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to convert server names to feature vectors\n",
    "cv = CountVectorizer(inputCol=\"servers_array\", outputCol=\"features\")\n",
    "cv_model = cv.fit(agg_df)\n",
    "cv_df = cv_model.transform(agg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d581b64d-9008-4365-8b4e-c447a1053158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Attributes (Vocabulary):\n",
      "0. null\n",
      "1. s5\n",
      "2. ps1\n",
      "3. s6\n",
      "4. ps3\n",
      "5. s9\n",
      "6. ps10\n",
      "7. p9\n",
      "8. s8\n",
      "9. ps4\n",
      "10. s4\n",
      "11. p1\n",
      "12. p8\n",
      "13. s2\n",
      "14. s7\n",
      "15. p10\n",
      "16. s10\n",
      "17. ps8\n",
      "18. p6\n",
      "19. p7\n",
      "20. ps9\n",
      "21. ps6\n",
      "22. s3\n",
      "23. p2\n",
      "24. ps5\n",
      "25. ps7\n",
      "26. ps2\n",
      "27. p3\n",
      "28. s1\n",
      "29. p5\n",
      "30. p4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Distinct Attributes (Vocabulary):\")\n",
    "for i, attr in enumerate(cv_model.vocabulary):\n",
    "    print(f\"{i}. {attr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38e3749c-28ce-41e6-86cb-410f95393300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(vec1, vec2):\n",
    "    set1 = set(vec1.indices)\n",
    "    set2 = set(vec2.indices)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "    return float(len(intersection)) / len(union)\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "# Merge overlapping groups\n",
    "def merge_groups(group_list):\n",
    "    groups = []\n",
    "    for group in group_list:\n",
    "        merged = False\n",
    "        for existing_group in groups:\n",
    "            if any(item in group for item in existing_group):\n",
    "                existing_group.update(group)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            groups.append(set(group))\n",
    "    return [list(group) for group in groups]\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "merge_groups_udf = udf(lambda x: merge_groups(x), ArrayType(ArrayType(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c92dd0e-18a8-4aa2-a9fb-495e1cd475ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply MinHash LSH\n",
    "numOftables = 10\n",
    "minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=numOftables)\n",
    "model = minhash.fit(cv_df)\n",
    "transformed_df = model.transform(cv_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1712416-7b36-4cf5-aaf1-f2b8a2cb0cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates\n",
      "+----------+----------+---------------+--------------------+--------------------+\n",
      "|processIdA|processIdB|JaccardDistance|           featuresA|           featuresB|\n",
      "+----------+----------+---------------+--------------------+--------------------+\n",
      "|      1823|      1824|            0.0|(31,[0,7,11,12,15...|(31,[0,7,11,12,15...|\n",
      "|      1825|      1826|            0.0|(31,[0,2,4,6,9,17...|(31,[0,2,4,6,9,17...|\n",
      "|      1821|      1822|            0.0|(31,[0,1,3,5,8,10...|(31,[0,1,3,5,8,10...|\n",
      "+----------+----------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "threshold=0.5\n",
    "candidates = model.approxSimilarityJoin(transformed_df, transformed_df, threshold , distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processId\").alias(\"processIdA\"),\n",
    "            col(\"datasetB.processId\").alias(\"processIdB\"),\n",
    "            col(\"JaccardDistance\"),col(\"datasetA.features\").alias(\"featuresA\"),col(\"datasetB.features\").alias(\"featuresB\"))\n",
    "candidates = candidates.filter(col(\"processIdA\") < col(\"processIdB\"))\n",
    "print(\"Candidates\")\n",
    "candidates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64bd6940-ffbf-47a4-8fa6-e1f4b31ab00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of groups is  3\n",
      "+---------+----------+--------+----+--------+\n",
      "|processId|FromServer|ToServer|time|  action|\n",
      "+---------+----------+--------+----+--------+\n",
      "|     1821|      null|      S1|   0| Request|\n",
      "|     1821|        s1|      S2|   1| Request|\n",
      "|     1821|        s2|      S3|   2| Request|\n",
      "|     1821|        s3|      S4|   3| Request|\n",
      "|     1821|        s4|      S5|   4| Request|\n",
      "|     1821|        s5|      S6|   5| Request|\n",
      "|     1821|        s6|      S7|   6| Request|\n",
      "|     1821|        s7|      S8|   7| Request|\n",
      "|     1821|        s8|      S9|   8| Request|\n",
      "|     1821|        s9|     S10|   9| Request|\n",
      "|     1821|       s10|      S9|  10|Response|\n",
      "|     1821|        s9|      S8|  11|Response|\n",
      "|     1821|        s8|      S7|  12|Response|\n",
      "|     1821|        s7|      S6|  13|Response|\n",
      "|     1821|        s6|      S5|  14|Response|\n",
      "|     1821|        s5|      S4|  15|Response|\n",
      "|     1821|        s4|      S3|  16|Response|\n",
      "|     1821|        s3|      S2|  17|Response|\n",
      "|     1821|        s2|      S1|  18|Response|\n",
      "|     1821|        s1|    null|  19|Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+----------+--------+----+--------+---------+\n",
      "|processId|FromServer|ToServer|time|  action|    Group|\n",
      "+---------+----------+--------+----+--------+---------+\n",
      "|     1821|      null|      S1|   0| Request|1821_1822|\n",
      "|     1821|        s1|      S2|   1| Request|1821_1822|\n",
      "|     1821|        s2|      S3|   2| Request|1821_1822|\n",
      "|     1821|        s3|      S4|   3| Request|1821_1822|\n",
      "|     1821|        s4|      S5|   4| Request|1821_1822|\n",
      "|     1821|        s5|      S6|   5| Request|1821_1822|\n",
      "|     1821|        s6|      S7|   6| Request|1821_1822|\n",
      "|     1821|        s7|      S8|   7| Request|1821_1822|\n",
      "|     1821|        s8|      S9|   8| Request|1821_1822|\n",
      "|     1821|        s9|     S10|   9| Request|1821_1822|\n",
      "|     1821|       s10|      S9|  10|Response|1821_1822|\n",
      "|     1821|        s9|      S8|  11|Response|1821_1822|\n",
      "|     1821|        s8|      S7|  12|Response|1821_1822|\n",
      "|     1821|        s7|      S6|  13|Response|1821_1822|\n",
      "|     1821|        s6|      S5|  14|Response|1821_1822|\n",
      "|     1821|        s5|      S4|  15|Response|1821_1822|\n",
      "|     1821|        s4|      S3|  16|Response|1821_1822|\n",
      "|     1821|        s3|      S2|  17|Response|1821_1822|\n",
      "|     1821|        s2|      S1|  18|Response|1821_1822|\n",
      "|     1821|        s1|    null|  19|Response|1821_1822|\n",
      "+---------+----------+--------+----+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Jaccard similarity for each candidate pair\n",
    "similarity_df = candidates.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"featuresA\"), col(\"featuresB\")))\n",
    "\n",
    "# Filter pairs with Jaccard similarity above a threshold (e.g., 70%)\n",
    "similarity_df = similarity_df.filter(col(\"JaccardSimilarity\") >= 0.7)\n",
    "grouped_df = similarity_df.groupBy(\"processIdA\").agg(collect_list(\"processIdB\").alias(\"similar_processIDs\"))\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processIdA\")), col(\"similar_processIDs\")))\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processIdA\").alias(\"group_representative\"))\n",
    "grouped_lists = exploded_df.groupBy(\"group_representative\") \\\n",
    "    .agg(collect_list(\"processID\").alias(\"group_list\")) \\\n",
    "    .agg(collect_list(\"group_list\").alias(\"group_lists\"))\n",
    "merged_groups = grouped_lists.withColumn(\"merged_groups\", merge_groups_udf(col(\"group_lists\"))) \\\n",
    "    .select(explode(col(\"merged_groups\")).alias(\"final_group\"))\n",
    "final_groups_df = merged_groups.select(concat_ws(\"_\", col(\"final_group\")).alias(\"Group\"), col(\"final_group\"))\n",
    "print(\"the number of groups is \",final_groups_df.count())\n",
    "output_path = \"./output/part2Observations.txt\"\n",
    "\n",
    "processes_from_groups = final_groups_df.selectExpr(\"explode(final_group) as processID\").distinct()\n",
    "# creating a dataframe only with the processes that were grouped.\n",
    "df_with_groups = dataForPart2.join(processes_from_groups, \"processID\", \"semi\")\n",
    "df_with_groups.show()\n",
    "\n",
    "exploded_final_groups_df = final_groups_df.select(\"Group\", explode(\"final_group\").alias(\"processID\"))\n",
    "joined_df = df_with_groups.join(exploded_final_groups_df, \"processID\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57da1ba-66bb-4a8d-93d6-d1ee0b51c4fc",
   "metadata": {},
   "source": [
    "# creating the txt files:\n",
    "## The desired files will be in the folder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "478ce4a9-238d-48ff-aabf-c8aaa58cc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_groups_to_txt(grouped_df, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        for row in grouped_df.collect():\n",
    "            group_name = row[\"Group\"]\n",
    "            process_ids = row[\"processIDs\"]\n",
    "            formatted_rows = row[\"formatted_rows\"]\n",
    "            \n",
    "            # Ensure process_ids are unique and sorted\n",
    "            process_ids = sorted(set(process_ids))\n",
    "            \n",
    "            file.write(f\"Group: {{{', '.join(map(str, process_ids))}}}\\n\")\n",
    "            \n",
    "            for process_id in process_ids:\n",
    "                file.write(f\"{process_id}:\\n\")\n",
    "                rows_for_process_id = [row for row in formatted_rows if row.endswith(f\",{process_id}>\")]\n",
    "                if rows_for_process_id:\n",
    "                    for formatted_row in rows_for_process_id:\n",
    "                        file.write(f\"{formatted_row}\\n\")\n",
    "                else:\n",
    "                    file.write(\"<No corresponding formatted rows found>\\n\")\n",
    "                    \n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08d1463f-638e-4f62-b897-713cd05eeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format each row into the desired format\n",
    "formatted_df = joined_df.withColumn(\n",
    "    \"formatted_row\",\n",
    "    concat_ws(\"\", lit(\"<\"), col(\"FromServer\"), lit(\",\"), col(\"ToServer\"),\n",
    "              lit(\",\"), col(\"time\"), lit(\",\"), col(\"action\"), lit(\",\"), col(\"processID\"), lit(\">\")))\n",
    "grouped_df = formatted_df.groupBy(\"Group\").agg(\n",
    "    collect_list(\"processID\").alias(\"processIDs\"),\n",
    "    collect_list(\"formatted_row\").alias(\"formatted_rows\"))\n",
    "\n",
    "output_path = \"./output/part2Observations.txt\"\n",
    "write_groups_to_txt(grouped_df, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e1ea4ce-19b9-4e19-8fca-7d9ca4738158",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2e950-229e-4871-9e23-efb5c5c0c7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
