{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8174202-3dc0-4790-b3d9-e30007a35ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "import ast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa9889-9d6d-4967-b91a-df72852f0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output file paths\n",
    "input_file = \"logs2.csv\"\n",
    "output_file = 'output2.csv'\n",
    "\n",
    "# Read the text file using CSV reader\n",
    "with open(input_file, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    data = [row for row in reader]\n",
    "\n",
    "# Define the CSV headers\n",
    "headers = ['FromServer', 'ToServer', 'time', 'action', 'processId']\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Data has been successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1e6b730-d548-4f15-aec2-4db854fdde2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------------------------------------+\n",
      "|processID|actions                                                      |\n",
      "+---------+-------------------------------------------------------------+\n",
      "|1        |[{null, lkVpiJ4, 0, Request}, {lkVpiJ4, null, 6, Response}]  |\n",
      "|6        |[{null, qZGv1, 27, Request}, {qZGv1, null, 36, Response}]    |\n",
      "|3        |[{null, OZBsEf0, 11, Request}, {OZBsEf0, null, 13, Response}]|\n",
      "|5        |[{null, Aum3, 22, Request}, {Aum3, null, 24, Response}]      |\n",
      "|4        |[{null, Aum3, 18, Request}, {Aum3, null, 28, Response}]      |\n",
      "|2        |[{null, lkVpiJ4, 9, Request}, {lkVpiJ4, null, 12, Response}] |\n",
      "+---------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, struct\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ProcessGrouping\").getOrCreate()\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "data_path = \"output2.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Collect the sequence of actions for each processID\n",
    "processes_df = df.groupBy(\"processID\").agg(collect_list(struct(\"FromServer\", \"ToServer\", \"time\", \"action\")).alias(\"actions\"))\n",
    "\n",
    "# Show the collected processes\n",
    "processes_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0445618-69e3-4355-9b62-cadabde26229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|processID|actions                                                      |actions_str                                     |shingles                                                                                                                                                                                                                                                                         |\n",
      "+---------+-------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1        |[{null, lkVpiJ4, 0, Request}, {lkVpiJ4, null, 6, Response}]  |null-lkVpiJ4-0-Request lkVpiJ4-null-6-Response  |[kVpiJ, ll-lk, espon, piJ4-, eques, uest , -null, Reque, 6-Res,  lkVp, spons, -Requ, l-lkV, 4-0-R, VpiJ4, ll-6-, l-6-R, lkVpi, ull-6, 0-Req, J4-nu, J4-0-, quest, st lk, iJ4-n, 4-nul, -lkVp, Respo, iJ4-0, ponse, ull-l, null-, est l, -Resp, -0-Re, -6-Re, t lkV]              |\n",
      "|6        |[{null, qZGv1, 27, Request}, {qZGv1, null, 36, Response}]    |null-qZGv1-27-Request qZGv1-null-36-Response    |[v1-27, 1-nul, l-36-, ull-3, 7-Req, t qZG, eques, uest , Gv1-n, -null, Reque, -qZGv, 6-Res, espon, spons, -Requ, Gv1-2, st qZ, qZGv1, -36-R, ull-q, est q, ll-qZ, quest, Respo,  qZGv, 1-27-, ponse, null-, l-qZG, -Resp, ll-36, 36-Re, 27-Re, -27-R, v1-nu, ZGv1-]              |\n",
      "|3        |[{null, OZBsEf0, 11, Request}, {OZBsEf0, null, 13, Response}]|null-OZBsEf0-11-Request OZBsEf0-null-13-Response|[-11-R, ZBsEf, espon, eques, uest , 13-Re, -null, Reque, spons, ll-13, 0-nul, -Requ, 1-Req, f0-11, OZBsE, ull-1, 0-11-, BsEf0, Ef0-n, sEf0-, 3-Res, ull-O, 11-Re, quest,  OZBs, ll-OZ, est O, Respo, -OZBs, t OZB, ponse, st OZ, null-, l-OZB, -Resp, -13-R, f0-nu, Ef0-1, l-13-]|\n",
      "|5        |[{null, Aum3, 22, Request}, {Aum3, null, 24, Response}]      |null-Aum3-22-Request Aum3-null-24-Response      |[ull-2, -Aum3, ll-24, espon, eques, uest , -null, Reque, spons, st Au, -24-R, -Requ, Aum3-, -22-R,  Aum3, um3-2, 4-Res, 22-Re, l-24-, 3-nul, l-Aum, ull-A, m3-22, quest, est A, t Aum, Respo, ll-Au, ponse, 24-Re, null-, um3-n, 3-22-, -Resp, m3-nu, 2-Req]                     |\n",
      "|4        |[{null, Aum3, 18, Request}, {Aum3, null, 28, Response}]      |null-Aum3-18-Request Aum3-null-28-Response      |[ull-2, -Aum3, espon, eques, uest , -null, m3-18, Reque, spons, st Au, -Requ, Aum3-, l-28-,  Aum3, -28-R, ll-28, 3-nul, l-Aum, ull-A, 8-Req, quest, est A, t Aum, 18-Re, 28-Re, Respo, um3-1, ll-Au, ponse, null-, um3-n, -18-R, 8-Res, 3-18-, -Resp, m3-nu]                     |\n",
      "|2        |[{null, lkVpiJ4, 9, Request}, {lkVpiJ4, null, 12, Response}] |null-lkVpiJ4-9-Request lkVpiJ4-null-12-Response |[-12-R, kVpiJ, ll-lk, espon, piJ4-, eques, uest , -null, Reque,  lkVp, spons, -Requ, l-lkV, VpiJ4, ull-1, lkVpi, 4-9-R, J4-nu, ll-12, quest, st lk, J4-9-, iJ4-n, 4-nul, 9-Req, -lkVp, Respo, ponse, ull-l, null-, 12-Re, est l, 2-Res, -Resp, -9-Re, iJ4-9, l-12-, t lkV]       |\n",
      "+---------+-------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# Define a UDF to convert actions into a string\n",
    "def actions_to_string(actions):\n",
    "    return \" \".join([f\"{action['FromServer']}-{action['ToServer']}-{action['time']}-{action['action']}\" for action in actions])\n",
    "\n",
    "actions_to_string_udf = udf(actions_to_string, StringType())\n",
    "\n",
    "# Add a column with the actions as strings\n",
    "processes_df = processes_df.withColumn(\"actions_str\", actions_to_string_udf(col(\"actions\")))\n",
    "\n",
    "# Convert the actions string into sets of shingles (substrings)\n",
    "def get_shingles(text, k=5):\n",
    "    return list(set([text[i:i+k] for i in range(len(text) - k + 1)]))\n",
    "\n",
    "get_shingles_udf = udf(lambda x: get_shingles(x), ArrayType(StringType()))\n",
    "\n",
    "processes_df = processes_df.withColumn(\"shingles\", get_shingles_udf(col(\"actions_str\")))\n",
    "\n",
    "# Show the DataFrame with shingles\n",
    "processes_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535a6028-763b-4aaa-807f-8b7538f16e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|processID|actions                                                      |actions_str                                     |shingles                                                                                                                                                                                                                                                                         |features                                                                                                                                                                                                                                                                                  |hashes                                                                                                                                               |\n",
      "+---------+-------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1        |[{null, lkVpiJ4, 0, Request}, {lkVpiJ4, null, 6, Response}]  |null-lkVpiJ4-0-Request lkVpiJ4-null-6-Response  |[kVpiJ, ll-lk, espon, piJ4-, eques, uest , -null, Reque, 6-Res,  lkVp, spons, -Requ, l-lkV, 4-0-R, VpiJ4, ll-6-, l-6-R, lkVpi, ull-6, 0-Req, J4-nu, J4-0-, quest, st lk, iJ4-n, 4-nul, -lkVp, Respo, iJ4-0, ponse, ull-l, null-, est l, -Resp, -0-Re, -6-Re, t lkV]              |(133,[0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19,21,22,24,25,26,27,29,31,35,39,40,41,43,50,59,70,72,82,85,101,110],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |[[4.5088506E7], [2.6162366E7], [6927342.0], [4.4860856E7], [2983014.0], [4.199357E7], [1567897.0], [5.2247873E7], [6.459435E7], [8.4067218E8]]       |\n",
      "|6        |[{null, qZGv1, 27, Request}, {qZGv1, null, 36, Response}]    |null-qZGv1-27-Request qZGv1-null-36-Response    |[v1-27, 1-nul, l-36-, ull-3, 7-Req, t qZG, eques, uest , Gv1-n, -null, Reque, -qZGv, 6-Res, espon, spons, -Requ, Gv1-2, st qZ, qZGv1, -36-R, ull-q, est q, ll-qZ, quest, Respo,  qZGv, 1-27-, ponse, null-, l-qZG, -Resp, ll-36, 36-Re, 27-Re, -27-R, v1-nu, ZGv1-]              |(133,[0,1,2,3,4,5,6,7,8,9,10,11,19,46,47,48,54,55,58,62,65,76,84,92,100,106,108,112,117,119,120,121,122,123,129,130,132],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])           |[[4.5088506E7], [2.6162366E7], [6.3089771E7], [3.9091643E7], [1.4870246E7], [4.1247007E7], [1567897.0], [2.3775726E7], [5.7698895E7], [6.06116716E8]]|\n",
      "|3        |[{null, OZBsEf0, 11, Request}, {OZBsEf0, null, 13, Response}]|null-OZBsEf0-11-Request OZBsEf0-null-13-Response|[-11-R, ZBsEf, espon, eques, uest , 13-Re, -null, Reque, spons, ll-13, 0-nul, -Requ, 1-Req, f0-11, OZBsE, ull-1, 0-11-, BsEf0, Ef0-n, sEf0-, 3-Res, ull-O, 11-Re, quest,  OZBs, ll-OZ, est O, Respo, -OZBs, t OZB, ponse, st OZ, null-, l-OZB, -Resp, -13-R, f0-nu, Ef0-1, l-13-]|(133,[0,1,2,3,4,5,6,7,8,9,10,11,15,45,52,56,60,61,66,67,68,73,78,79,81,83,91,93,97,102,104,105,109,114,115,118,125,127,128],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[[4.5088506E7], [1.14021471E8], [2.2160134E7], [4.4860856E7], [3.864471E7], [8.4002412E7], [1567897.0], [1.09192167E8], [3.701253E7], [6.48763164E8]]|\n",
      "|5        |[{null, Aum3, 22, Request}, {Aum3, null, 24, Response}]      |null-Aum3-22-Request Aum3-null-24-Response      |[ull-2, -Aum3, ll-24, espon, eques, uest , -null, Reque, spons, st Au, -24-R, -Requ, Aum3-, -22-R,  Aum3, um3-2, 4-Res, 22-Re, l-24-, 3-nul, l-Aum, ull-A, m3-22, quest, est A, t Aum, Respo, ll-Au, ponse, 24-Re, null-, um3-n, 3-22-, -Resp, m3-nu, 2-Req]                     |(133,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,20,23,28,30,32,33,34,36,37,38,42,63,75,87,89,94,99,103,107,111,126],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |[[1.8056783E7], [4.2343048E7], [6.3089771E7], [4986711.0], [2.6757478E7], [4.0500444E7], [1567897.0], [5.0410458E7], [7.1489805E7], [6.70086388E8]]  |\n",
      "|4        |[{null, Aum3, 18, Request}, {Aum3, null, 28, Response}]      |null-Aum3-18-Request Aum3-null-28-Response      |[ull-2, -Aum3, espon, eques, uest , -null, m3-18, Reque, spons, st Au, -Requ, Aum3-, l-28-,  Aum3, -28-R, ll-28, 3-nul, l-Aum, ull-A, 8-Req, quest, est A, t Aum, 18-Re, 28-Re, Respo, um3-1, ll-Au, ponse, null-, um3-n, -18-R, 8-Res, 3-18-, -Resp, m3-nu]                     |(133,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,20,23,28,30,32,33,34,36,37,38,44,51,57,77,86,88,95,96,98,113,131],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                             |[[1.8056783E7], [1.14021471E8], [6.3089771E7], [4986711.0], [6.9982217E7], [4.0500444E7], [1567897.0], [5.0410458E7], [4.3907985E7], [6.16778328E8]] |\n",
      "|2        |[{null, lkVpiJ4, 9, Request}, {lkVpiJ4, null, 12, Response}] |null-lkVpiJ4-9-Request lkVpiJ4-null-12-Response |[-12-R, kVpiJ, ll-lk, espon, piJ4-, eques, uest , -null, Reque,  lkVp, spons, -Requ, l-lkV, VpiJ4, ull-1, lkVpi, 4-9-R, J4-nu, ll-12, quest, st lk, J4-9-, iJ4-n, 4-nul, 9-Req, -lkVp, Respo, ponse, ull-l, null-, 12-Re, est l, 2-Res, -Resp, -9-Re, iJ4-9, l-12-, t lkV]       |(133,[0,1,2,3,4,5,6,7,8,9,10,11,15,16,17,18,21,22,24,25,26,27,29,31,35,39,40,41,49,53,64,69,71,74,80,90,116,124],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |[[4.5088506E7], [3.4252707E7], [6927342.0], [2.2039177E7], [2983014.0], [1.25264691E8], [1567897.0], [2.1938311E7], [3.0117075E7], [6.91409612E8]]   |\n",
      "+---------+-------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH, CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Use CountVectorizer to convert shingles into vectors\n",
    "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\")\n",
    "cv_model = cv.fit(processes_df)\n",
    "vectorized_df = cv_model.transform(processes_df)\n",
    "\n",
    "# Apply MinHashLSH\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "mh_model = mh.fit(vectorized_df)\n",
    "hashed_df = mh_model.transform(vectorized_df)\n",
    "\n",
    "# Show the DataFrame with MinHash hashes\n",
    "hashed_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8789ad54-7dde-413c-9453-a6b9ed4076a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+--------+\n",
      "|processID|FromServer|ToServer|time|  action|\n",
      "+---------+----------+--------+----+--------+\n",
      "|        1|      null| lkVpiJ4|   0| Request|\n",
      "|        1|   lkVpiJ4|    null|   6|Response|\n",
      "|        2|      null| lkVpiJ4|   9| Request|\n",
      "|        1|      null| lkVpiJ4|   9| Request|\n",
      "|        2|   lkVpiJ4|    null|  12|Response|\n",
      "|        1|   lkVpiJ4|    null|  12|Response|\n",
      "|        2|      null| OZBsEf0|  11| Request|\n",
      "|        2|   OZBsEf0|    null|  13|Response|\n",
      "|        4|      null|    Aum3|  18| Request|\n",
      "|        4|      Aum3|    null|  28|Response|\n",
      "|        4|      null|    Aum3|  22| Request|\n",
      "|        4|      Aum3|    null|  24|Response|\n",
      "|        1|      null|   qZGv1|  27| Request|\n",
      "|        1|     qZGv1|    null|  36|Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, array_union\n",
    "\n",
    "# Group by processID_A and collect the similar processIDs\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "\n",
    "# Convert processID_A to an array and concatenate with similar_processIDs\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "\n",
    "# Find the first process in each group\n",
    "first_processes_df = grouped_df.withColumn(\"first_processID\", col(\"processID_A\"))\n",
    "\n",
    "# Replace processIDs in the original DataFrame\n",
    "# We will use a join to map the original process IDs to their group representative (first process ID)\n",
    "# First, explode the all_processIDs array to get a mapping of each process ID to its group\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"first_processID\"))\n",
    "\n",
    "# Join with the original DataFrame to replace processIDs with their group representative\n",
    "replaced_df = df.join(exploded_df, on=\"processID\", how=\"left\") \\\n",
    "                .select(col(\"first_processID\"), col(\"FromServer\"), col(\"ToServer\"), col(\"time\"), col(\"action\")) \\\n",
    "                .withColumnRenamed(\"first_processID\", \"processID\")\n",
    "\n",
    "# Show the replaced DataFrame\n",
    "replaced_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbf9177e-9182-47ef-a38c-7d08acc78f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|processID_A|Group|\n",
      "+-----------+-----+\n",
      "|1          |1,2,6|\n",
      "|4          |4,5  |\n",
      "|2          |2,3  |\n",
      "+-----------+-----+\n",
      "\n",
      "+---------+----------+--------+----+--------+\n",
      "|processID|FromServer|ToServer|time|  action|\n",
      "+---------+----------+--------+----+--------+\n",
      "|        1|      null| lkVpiJ4|   0| Request|\n",
      "|        1|   lkVpiJ4|    null|   6|Response|\n",
      "|        2|      null| lkVpiJ4|   9| Request|\n",
      "|        1|      null| lkVpiJ4|   9| Request|\n",
      "|        2|   lkVpiJ4|    null|  12|Response|\n",
      "|        1|   lkVpiJ4|    null|  12|Response|\n",
      "|        2|      null| OZBsEf0|  11| Request|\n",
      "|        2|   OZBsEf0|    null|  13|Response|\n",
      "|        4|      null|    Aum3|  18| Request|\n",
      "|        4|      Aum3|    null|  28|Response|\n",
      "|        4|      null|    Aum3|  22| Request|\n",
      "|        4|      Aum3|    null|  24|Response|\n",
      "|        1|      null|   qZGv1|  27| Request|\n",
      "|        1|     qZGv1|    null|  36|Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, array_union, concat_ws, explode\n",
    "\n",
    "# Group by processID_A and collect the similar processIDs\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "\n",
    "# Convert processID_A to an array and concatenate with similar_processIDs\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "\n",
    "# Find the first process in each group\n",
    "first_processes_df = grouped_df.withColumn(\"first_processID\", col(\"processID_A\"))\n",
    "\n",
    "# Convert the all_processIDs array to a string for CSV output\n",
    "groups_df = grouped_df.select(\"processID_A\", concat_ws(\",\", col(\"all_processIDs\")).alias(\"Group\"))\n",
    "\n",
    "# Show the groups DataFrame\n",
    "groups_df.show(truncate=False)\n",
    "\n",
    "# Write the groups to a file\n",
    "# groups_df.write.csv(\"path_to_output_groups_file.csv\", header=True)\n",
    "\n",
    "# Explode the all_processIDs array to get a mapping of each process ID to its group\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"first_processID\"))\n",
    "\n",
    "# Join with the original DataFrame to replace processIDs with their group representative\n",
    "replaced_df = df.join(exploded_df, on=\"processID\", how=\"left\") \\\n",
    "                .select(col(\"first_processID\"), col(\"FromServer\"), col(\"ToServer\"), col(\"time\"), col(\"action\")) \\\n",
    "                .withColumnRenamed(\"first_processID\", \"processID\")\n",
    "\n",
    "# Show the replaced DataFrame\n",
    "replaced_df.show()\n",
    "\n",
    "# Write the replaced DataFrame to a file\n",
    "# replaced_df.write.csv(\"path_to_output_replaced_file.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c7997-83a4-46ac-81c1-45c9935dff0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "350bbde8-4574-4c28-afa6-140c8dc6142e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+-------+\n",
      "|processID|FromServer|ToServer|time| action|\n",
      "+---------+----------+--------+----+-------+\n",
      "|        1|      null| lkVpiJ4|   0|Request|\n",
      "|        4|      null|    Aum3|  18|Request|\n",
      "|        2|      null| lkVpiJ4|   9|Request|\n",
      "+---------+----------+--------+----+-------+\n",
      "\n",
      "+-----------+-----+\n",
      "|processID_A|Group|\n",
      "+-----------+-----+\n",
      "|1          |1,2,6|\n",
      "|4          |4,5  |\n",
      "|2          |2,3  |\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, struct, array, array_union, concat_ws, explode, min\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcessGrouping\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "data_path = \"output2.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Group by processID and collect the sequence of actions\n",
    "processes_df = df.groupBy(\"processID\").agg(collect_list(struct(\"FromServer\", \"ToServer\", \"time\", \"action\")).alias(\"actions\"))\n",
    "\n",
    "# Convert actions to string for MinHash LSH\n",
    "def actions_to_string(actions):\n",
    "    return \" \".join([f\"{action['FromServer']}-{action['ToServer']}-{action['time']}-{action['action']}\" for action in actions])\n",
    "\n",
    "actions_to_string_udf = udf(actions_to_string, StringType())\n",
    "processes_df = processes_df.withColumn(\"actions_str\", actions_to_string_udf(col(\"actions\")))\n",
    "\n",
    "# Convert actions string into shingles\n",
    "def get_shingles(text, k=5):\n",
    "    return list(set([text[i:i+k] for i in range(len(text) - k + 1)]))\n",
    "\n",
    "get_shingles_udf = udf(lambda x: get_shingles(x), ArrayType(StringType()))\n",
    "processes_df = processes_df.withColumn(\"shingles\", get_shingles_udf(col(\"actions_str\")))\n",
    "\n",
    "# Use CountVectorizer to convert shingles into vectors\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\")\n",
    "cv_model = cv.fit(processes_df)\n",
    "vectorized_df = cv_model.transform(processes_df)\n",
    "\n",
    "# Apply MinHashLSH\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "mh_model = mh.fit(vectorized_df)\n",
    "hashed_df = mh_model.transform(vectorized_df)\n",
    "\n",
    "# Find similar process IDs using MinHashLSH\n",
    "similarity_df = mh_model.approxSimilarityJoin(hashed_df, hashed_df, 0.8, distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processID\").alias(\"processID_A\"),\n",
    "            col(\"datasetB.processID\").alias(\"processID_B\"),\n",
    "            col(\"JaccardDistance\"))\n",
    "\n",
    "# Filter out self-joins and duplicates\n",
    "similarity_df = similarity_df.filter(col(\"processID_A\") < col(\"processID_B\"))\n",
    "\n",
    "# Group by processID_A and collect similar processIDs\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "\n",
    "# Convert processID_A to an array and concatenate with similar_processIDs\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "\n",
    "# Find the first process in each group\n",
    "grouped_df = grouped_df.withColumn(\"first_processID\", col(\"processID_A\"))\n",
    "\n",
    "# Explode the all_processIDs array to get a mapping of each process ID to its group\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"first_processID\"))\n",
    "\n",
    "# Join with the original DataFrame to filter out all but the first process for each group\n",
    "filtered_df = df.join(exploded_df, on=\"processID\", how=\"left\") \\\n",
    "    .groupBy(\"first_processID\") \\\n",
    "    .agg(min(\"time\").alias(\"min_time\")) \\\n",
    "    .join(df, (df[\"processID\"] == col(\"first_processID\")) & (df[\"time\"] == col(\"min_time\")), \"inner\") \\\n",
    "    .select(col(\"first_processID\").alias(\"processID\"), col(\"FromServer\"), col(\"ToServer\"), col(\"time\"), col(\"action\"))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show()\n",
    "\n",
    "# Convert the all_processIDs array to a string for CSV output\n",
    "groups_df = grouped_df.select(\"processID_A\", concat_ws(\",\", col(\"all_processIDs\")).alias(\"Group\"))\n",
    "\n",
    "# Show the groups DataFrame\n",
    "groups_df.show(truncate=False)\n",
    "\n",
    "# # Write the groups to a file\n",
    "# groups_df.write.csv(\"part1Observations.txt\", header=True)\n",
    "\n",
    "# # Write the filtered DataFrame to a file\n",
    "# filtered_df.write.csv(\"part1Output.txt\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "133167f9-53cc-417c-a5ba-5b5333827a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+\n",
      "|  Group|representative_processID|\n",
      "+-------+------------------------+\n",
      "|    4_5|                       4|\n",
      "|1_2_3_6|                       1|\n",
      "+-------+------------------------+\n",
      "\n",
      "+---------+----------+--------+----+--------+\n",
      "|processID|FromServer|ToServer|time|action  |\n",
      "+---------+----------+--------+----+--------+\n",
      "|1        |null      |lkVpiJ4 |0   |Request |\n",
      "|1        |lkVpiJ4   |null    |6   |Response|\n",
      "|4        |null      |Aum3    |18  |Request |\n",
      "|4        |Aum3      |null    |28  |Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "\n",
      "+-------+------------+\n",
      "|  Group| final_group|\n",
      "+-------+------------+\n",
      "|1_2_3_6|[1, 2, 3, 6]|\n",
      "|    4_5|      [4, 5]|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##final\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, struct, array, array_union, concat_ws, explode, min, udf, split, lit\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcessGrouping\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "data_path = \"output2.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Group by processID and collect the sequence of actions\n",
    "processes_df = df.groupBy(\"processID\").agg(collect_list(struct(\"FromServer\", \"ToServer\", \"time\", \"action\")).alias(\"actions\"))\n",
    "\n",
    "# Convert actions to string for MinHash LSH\n",
    "def actions_to_string(actions):\n",
    "    return \" \".join([f\"{action['FromServer']}-{action['ToServer']}-{action['time']}-{action['action']}\" for action in actions])\n",
    "\n",
    "actions_to_string_udf = udf(actions_to_string, StringType())\n",
    "processes_df = processes_df.withColumn(\"actions_str\", actions_to_string_udf(col(\"actions\")))\n",
    "\n",
    "# Convert actions string into shingles\n",
    "def get_shingles(text, k=5):\n",
    "    return list(set([text[i:i+k] for i in range(len(text) - k + 1)]))\n",
    "\n",
    "get_shingles_udf = udf(lambda x: get_shingles(x), ArrayType(StringType()))\n",
    "processes_df = processes_df.withColumn(\"shingles\", get_shingles_udf(col(\"actions_str\")))\n",
    "\n",
    "# Use CountVectorizer to convert shingles into vectors\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\")\n",
    "cv_model = cv.fit(processes_df)\n",
    "vectorized_df = cv_model.transform(processes_df)\n",
    "\n",
    "# Apply MinHashLSH\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "mh_model = mh.fit(vectorized_df)\n",
    "hashed_df = mh_model.transform(vectorized_df)\n",
    "\n",
    "# Find similar process IDs using MinHashLSH\n",
    "similarity_df = mh_model.approxSimilarityJoin(hashed_df, hashed_df, 0.8, distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processID\").alias(\"processID_A\"),\n",
    "            col(\"datasetB.processID\").alias(\"processID_B\"),\n",
    "            col(\"JaccardDistance\"))\n",
    "\n",
    "# Filter out self-joins and duplicates\n",
    "similarity_df = similarity_df.filter(col(\"processID_A\") < col(\"processID_B\"))\n",
    "\n",
    "# Group by processID_A and collect similar processIDs\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "\n",
    "# Convert processID_A to an array and concatenate with similar_processIDs\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "\n",
    "# Explode the all_processIDs array to get a mapping of each process ID to its group\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"group_representative\"))\n",
    "\n",
    "# Merge overlapping groups\n",
    "def merge_groups(group_list):\n",
    "    groups = []\n",
    "    for group in group_list:\n",
    "        merged = False\n",
    "        for existing_group in groups:\n",
    "            if any(item in group for item in existing_group):\n",
    "                existing_group.update(group)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            groups.append(set(group))\n",
    "    return [list(group) for group in groups]\n",
    "\n",
    "merge_groups_udf = udf(lambda x: merge_groups(x), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "grouped_lists = exploded_df.groupBy(\"group_representative\") \\\n",
    "    .agg(collect_list(\"processID\").alias(\"group_list\")) \\\n",
    "    .agg(collect_list(\"group_list\").alias(\"group_lists\"))\n",
    "\n",
    "merged_groups = grouped_lists.withColumn(\"merged_groups\", merge_groups_udf(col(\"group_lists\"))) \\\n",
    "    .select(explode(col(\"merged_groups\")).alias(\"final_group\"))\n",
    "\n",
    "# Convert the final groups to a DataFrame\n",
    "final_groups_df = merged_groups.select(concat_ws(\"_\", col(\"final_group\")).alias(\"Group\"), col(\"final_group\"))\n",
    "\n",
    "# Find the representative process for each final group\n",
    "final_groups_exploded = final_groups_df.withColumn(\"processID\", explode(col(\"final_group\")))\n",
    "\n",
    "# Join with the original DataFrame to keep only the representative process\n",
    "filtered_df = df.join(final_groups_exploded, on=\"processID\", how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Select the smallest processID in each group as the representative\n",
    "group_representative_df = final_groups_exploded.groupBy(\"Group\").agg(min(\"processID\").alias(\"representative_processID\"))\n",
    "group_representative_df.show()\n",
    "# Join to get the full details of the representative processes\n",
    "representative_processes_df = group_representative_df.join(filtered_df, filtered_df[\"processID\"] == group_representative_df.representative_processID, \"inner\") \\\n",
    "    .select(\"processID\", \"FromServer\", \"ToServer\", \"time\", \"action\")\n",
    "\n",
    "\n",
    "# Show the final result\n",
    "representative_processes_df.show(truncate=False)\n",
    "# Convert group details to the desired format\n",
    "def format_group(group):\n",
    "    group_id = group[0]\n",
    "    processes = group[1]\n",
    "    group_header = f\"Group:{{{group_id}}}\"\n",
    "    process_details = \"\\n\".join([f\"processID{process['processID']}:\\n{process}\" for process in processes])\n",
    "    return f\"{group_header}\\n{process_details}\"\n",
    "\n",
    "final_groups_df.show()\n",
    "# Write the final groups to a file\n",
    "# final_groups_df.write.csv(\"path_to_output_groups_file.csv\", header=True)\n",
    "\n",
    "# # Write the filtered DataFrame to a file\n",
    "# representative_processes_df.write.csv(\"path_to_output_filtered_file.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039979eb-7a90-4a05-9cdc-039be9d40787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bde08-c34c-482d-a596-5704940c18f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1259e-ffbc-44a2-834a-d0db130995a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c1e177-41e7-47a3-9cf5-986a81caffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder\\\n",
    "#         .master(\"local\")\\\n",
    "#             .appName(\"test\")\\\n",
    "#                 .getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\")\\\n",
    "            .appName(\"YourAppName\")\\\n",
    "                .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "                    .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "                        .config(\"spark.network.timeout\", \"60000s\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d476b-9667-4f47-a452-1900df5430a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db53bfc-df18-426d-90c5-861d6ed5115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to output2.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6027c4ca-b3ec-45b8-8a7a-c6ca4127f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+--------+---------+\n",
      "|FromServer|ToServer|time|  action|processId|\n",
      "+----------+--------+----+--------+---------+\n",
      "|      null| lkVpiJ4|   0| Request|        1|\n",
      "|   lkVpiJ4|    null|   6|Response|        1|\n",
      "|      null| lkVpiJ4|   9| Request|        2|\n",
      "|   lkVpiJ4|    null|  12|Response|        2|\n",
      "|      null| OZBsEf0|  11| Request|        3|\n",
      "|   OZBsEf0|    null|  13|Response|        3|\n",
      "|      null|    Aum3|  18| Request|        4|\n",
      "|      Aum3|    null|  28|Response|        4|\n",
      "|      null|    Aum3|  22| Request|        5|\n",
      "|      Aum3|    null|  24|Response|        5|\n",
      "|      null|   qZGv1|  27| Request|        6|\n",
      "|     qZGv1|    null|  36|Response|        6|\n",
      "+----------+--------+----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the data frame\n",
    "import pandas as pd\n",
    "df = spark.read.csv(\"output2.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf5f637-e937-40a5-a65b-87023c60ecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|processId|         Shingle-set|\n",
      "+---------+--------------------+\n",
      "|        1|[nulll, ulllk, ll...|\n",
      "|        6|[nullq, ullqZ, ll...|\n",
      "|        3|[nullO, ullOZ, ll...|\n",
      "|        5|[nullA, ullAu, ll...|\n",
      "|        4|[nullA, ullAu, ll...|\n",
      "|        2|[nulll, ulllk, ll...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create k-shingles\n",
    "# Step 1: Set up PySpark\n",
    "from pyspark.sql.functions import col, collect_list, lit, udf, explode, array, min as spark_min , struct, collect_set , concat_ws\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "\n",
    "\n",
    "# Define function to create k-shingles for each process\n",
    "def create_k_shingles(row, k):\n",
    "    concatenated_str = ''.join(row)\n",
    "    shingles = [concatenated_str[i:i+k] for i in range(len(concatenated_str) - (k - 1))]\n",
    "    return shingles\n",
    "\n",
    "# User Defined Function (UDF) to apply the function to DataFrame\n",
    "create_shingles_udf = udf(create_k_shingles, ArrayType(StringType()))\n",
    "\n",
    "# Create DataFrame with process ID and its shingle set\n",
    "shingles_df = df.groupBy(\"processId\") \\\n",
    "                .agg(collect_list(concat_ws(\"\", df.FromServer, df.ToServer)).alias(\"concatenated_str\")) \\\n",
    "                .withColumn(\"Shingle-set\", create_shingles_udf(\"concatenated_str\", lit(5))) \\\n",
    "                .select(\"processId\", \"Shingle-set\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shingles_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf18348-3219-4b77-bf60-7e00626c5253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------+\n",
      "|processId|hashed_shingles                                       |\n",
      "+---------+------------------------------------------------------+\n",
      "|1        |[96, 89, 83, 3, 4, 11, 47, 41, 27, 64, 56, 57, 79, 43]|\n",
      "|6        |[77, 84, 70, 100, 35, 71, 57, 22, 29, 47, 95, 59]     |\n",
      "|3        |[52, 46, 71, 53, 83, 17, 21, 3, 54, 18, 28, 75, 98]   |\n",
      "|5        |[80, 70, 100, 56, 28, 94, 55, 47, 26, 98, 8]          |\n",
      "|4        |[80, 70, 100, 56, 28, 94, 55, 47, 26, 98, 8]          |\n",
      "|2        |[96, 89, 83, 3, 4, 11, 47, 41, 27, 64, 56, 57, 79, 43]|\n",
      "+---------+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, explode, collect_set\n",
    "from pyspark.sql.types import IntegerType\n",
    "import hashlib\n",
    "\n",
    "# Define the number of buckets\n",
    "num_buckets = 100\n",
    "\n",
    "# Define hash function to hash shingles to bucket numbers\n",
    "def hash_shingle(shingle):\n",
    "    # Create a hash object\n",
    "    hash_object = hashlib.md5(shingle.encode())\n",
    "    # Convert the hash to an integer\n",
    "    hash_int = int(hash_object.hexdigest(), 16)\n",
    "    # Map the integer to a bucket number\n",
    "    bucket = (hash_int % num_buckets) + 1  # Adding 1 to ensure bucket numbers start from 1\n",
    "    return bucket\n",
    "\n",
    "# User Defined Function (UDF) to apply the hash function to each shingle\n",
    "hash_shingle_udf = udf(hash_shingle, IntegerType())\n",
    "\n",
    "# Explode the array elements to separate rows\n",
    "exploded_df = shingles_df.withColumn(\"Shingle\", explode(\"Shingle-set\"))\n",
    "\n",
    "# Hash each shingle\n",
    "hashed_shingles_df = exploded_df.withColumn(\"hashed_shingle\", hash_shingle_udf(col(\"Shingle\"))) \\\n",
    "                                .groupBy(\"processId\") \\\n",
    "                                .agg(collect_set(\"hashed_shingle\").alias(\"hashed_shingles\"))\n",
    "\n",
    "# Display the hashed shingles for each process\n",
    "hashed_shingles_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf6a0bd-3832-4ee3-8ccd-d42d9126d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "|processId|bucket_1|bucket_2|bucket_3|bucket_4|bucket_5|bucket_6|bucket_7|bucket_8|bucket_9|bucket_10|bucket_11|bucket_12|bucket_13|bucket_14|bucket_15|bucket_16|bucket_17|bucket_18|bucket_19|bucket_20|bucket_21|bucket_22|bucket_23|bucket_24|bucket_25|bucket_26|bucket_27|bucket_28|bucket_29|bucket_30|bucket_31|bucket_32|bucket_33|bucket_34|bucket_35|bucket_36|bucket_37|bucket_38|bucket_39|bucket_40|bucket_41|bucket_42|bucket_43|bucket_44|bucket_45|bucket_46|bucket_47|bucket_48|bucket_49|bucket_50|bucket_51|bucket_52|bucket_53|bucket_54|bucket_55|bucket_56|bucket_57|bucket_58|bucket_59|bucket_60|bucket_61|bucket_62|bucket_63|bucket_64|bucket_65|bucket_66|bucket_67|bucket_68|bucket_69|bucket_70|bucket_71|bucket_72|bucket_73|bucket_74|bucket_75|bucket_76|bucket_77|bucket_78|bucket_79|bucket_80|bucket_81|bucket_82|bucket_83|bucket_84|bucket_85|bucket_86|bucket_87|bucket_88|bucket_89|bucket_90|bucket_91|bucket_92|bucket_93|bucket_94|bucket_95|bucket_96|bucket_97|bucket_98|bucket_99|bucket_100|\n",
      "+---------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "|1        |0       |0       |1       |1       |0       |0       |0       |0       |0       |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |1        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |1        |1        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |1        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0         |\n",
      "|6        |0       |0       |0       |0       |0       |0       |0       |0       |0       |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |1        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |1         |\n",
      "|3        |0       |0       |1       |0       |0       |0       |0       |0       |0       |0        |0        |0        |0        |0        |0        |0        |1        |1        |0        |0        |1        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |1        |1        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0         |\n",
      "|5        |0       |0       |0       |0       |0       |0       |0       |1       |0       |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |1        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |1        |0        |1         |\n",
      "|4        |0       |0       |0       |0       |0       |0       |0       |1       |0       |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |1        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |1        |0        |1         |\n",
      "|2        |0       |0       |1       |1       |0       |0       |0       |0       |0       |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |1        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |1        |1        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |1        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0        |0        |0        |1        |0        |0        |0        |0         |\n",
      "+---------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the characteristic matrix where the rows will be the processes and the columns the bucket numbers of the hashed shingles\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "all_buckets = [i for i in range(1, num_buckets + 1)]\n",
    "\n",
    "# Define a function to check if a shingle is present in a set of hashed shingles\n",
    "def check_shingle_presence(hashed_shingles, bucket):\n",
    "    return when(array_contains(hashed_shingles, bucket), 1).otherwise(0)\n",
    "\n",
    "# Create a column for each bucket number indicating its presence in the hashed shingles\n",
    "characteristic_matrix = hashed_shingles_df.select(\n",
    "    col(\"processId\"),\n",
    "    *[check_shingle_presence(col(\"hashed_shingles\"), lit(bucket)).alias(f\"bucket_{bucket}\") for bucket in all_buckets]\n",
    ")\n",
    "\n",
    "# Display the characteristic matrix\n",
    "characteristic_matrix.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87a58f1-45a9-4fdf-a768-a5615ee3358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------------------------------------------------------------------------------+\n",
      "|processId|features                                                                                                 |\n",
      "+---------+---------------------------------------------------------------------------------------------------------+\n",
      "|1        |(100,[3,4,11,27,41,43,47,56,57,64,79,83,89,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|6        |(100,[22,29,35,47,57,59,70,71,77,84,95],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |\n",
      "|3        |(100,[3,17,18,21,28,46,52,53,54,71,75,83,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])      |\n",
      "|5        |(100,[8,26,28,47,55,56,70,80,94,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |\n",
      "|4        |(100,[8,26,28,47,55,56,70,80,94,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |\n",
      "|2        |(100,[3,4,11,27,41,43,47,56,57,64,79,83,89,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+---------+---------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert characteristic matrix to sparse vectors because the MInhashLSH takes vectors as an input\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert rows to sparse vectors\n",
    "def row_to_sparse_vector(row):\n",
    "    process_id = row[\"processId\"]\n",
    "    features = [row[f\"bucket_{i}\"] for i in range(1, num_buckets)]\n",
    "    indices = [i+1 for i, feature in enumerate(features) if feature == 1]\n",
    "    values = [1.0] * len(indices)\n",
    "    sparse_vector = Vectors.sparse(num_buckets, indices, values)\n",
    "    return (process_id, sparse_vector)\n",
    "\n",
    "# Convert DataFrame to RDD and then to DataFrame with sparse vectors\n",
    "sparse_vectors_rdd = characteristic_matrix.rdd.map(row_to_sparse_vector)\n",
    "sparse_vectors_df = spark.createDataFrame(sparse_vectors_rdd, [\"processId\", \"features\"])\n",
    "\n",
    "# Show the DataFrame with sparse vectors\n",
    "sparse_vectors_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea6542f-d71a-4aa0-941d-b514fcc6471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------------------------------------------------------------------------------+----------------+\n",
      "|processId|features                                                                                                 |hashes          |\n",
      "+---------+---------------------------------------------------------------------------------------------------------+----------------+\n",
      "|1        |(100,[3,4,11,27,41,43,47,56,57,64,79,83,89,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[[1.77527643E8]]|\n",
      "|6        |(100,[22,29,35,47,57,59,70,71,77,84,95],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |[[1.09108826E8]]|\n",
      "|3        |(100,[3,17,18,21,28,46,52,53,54,71,75,83,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])      |[[1.1662387E7]] |\n",
      "|5        |(100,[8,26,28,47,55,56,70,80,94,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |[[6.9717631E7]] |\n",
      "|4        |(100,[8,26,28,47,55,56,70,80,94,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |[[6.9717631E7]] |\n",
      "|2        |(100,[3,4,11,27,41,43,47,56,57,64,79,83,89,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[[1.77527643E8]]|\n",
      "+---------+---------------------------------------------------------------------------------------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ft the MinHash model\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "# Create MinHashLSH model with 3 hash tables (bands)\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\",seed=123456 ,numHashTables=1) # In PySpark, you control the number of bands via numHashTables, and the implementation ensures the MinHash signatures are of appropriate length to be divided into these bands.\n",
    "\n",
    "# Fit the model\n",
    "model = mh.fit(sparse_vectors_df)\n",
    "\n",
    "# Transform the data to get the MinHash signatures\n",
    "transformed_df = model.transform(sparse_vectors_df)\n",
    "transformed_df.show(truncate=False)\n",
    "\n",
    "#12 --> 123456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8751080-8659-440a-ae71-66f4e291498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+\n",
      "|processIdA|processIdB|JaccardDistance|\n",
      "+----------+----------+---------------+\n",
      "|1         |2         |0.0            |\n",
      "|4         |5         |0.0            |\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Find candidate pairs\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Find candidate pairs\n",
    "# Use approxSimilarityJoin to find candidate pairs\n",
    "candidate_pairs = model.approxSimilarityJoin(transformed_df, transformed_df,1, distCol=\"JaccardDistance\")\n",
    "\n",
    "\n",
    "# Filter out self-comparisons and duplicate pairs\n",
    "distinct_pairs = candidate_pairs.filter(col(\"datasetA.processId\") < col(\"datasetB.processId\"))\n",
    "\n",
    "# Show distinct candidate pairs\n",
    "distinct_pairs.select(\n",
    "    col(\"datasetA.processId\").alias(\"processIdA\"),\n",
    "    col(\"datasetB.processId\").alias(\"processIdB\"),\n",
    "    col(\"JaccardDistance\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ef1b1b-9798-4186-a530-aac58edce01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------------+\n",
      "|processId|            features|          hashes|\n",
      "+---------+--------------------+----------------+\n",
      "|        1|(100,[3,4,11,27,4...|[[1.77527643E8]]|\n",
      "|        6|(100,[22,29,35,47...|[[1.09108826E8]]|\n",
      "|        3|(100,[3,17,18,21,...| [[1.1662387E7]]|\n",
      "|        5|(100,[8,26,28,47,...| [[6.9717631E7]]|\n",
      "|        4|(100,[8,26,28,47,...| [[6.9717631E7]]|\n",
      "|        2|(100,[3,4,11,27,4...|[[1.77527643E8]]|\n",
      "+---------+--------------------+----------------+\n",
      "\n",
      "root\n",
      " |-- processId: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- hashes: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      "\n",
      "+----------+----------+---------------+\n",
      "|processIdA|processIdB|JaccardDistance|\n",
      "+----------+----------+---------------+\n",
      "|1         |2         |0.0            |\n",
      "|4         |5         |0.0            |\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Make sure the transformed_df has the necessary columns and correct types\n",
    "transformed_df.show()\n",
    "transformed_df.printSchema()\n",
    "\n",
    "# Find candidate pairs\n",
    "# Use approxSimilarityJoin to find candidate pairs\n",
    "candidate_pairs = model.approxSimilarityJoin(transformed_df, transformed_df, 1.0, distCol=\"JaccardDistance\")\n",
    "\n",
    "# Filter out self-comparisons and duplicate pairs\n",
    "distinct_pairs = candidate_pairs.filter(col(\"datasetA.processId\") < col(\"datasetB.processId\"))\n",
    "\n",
    "# Show distinct candidate pairs\n",
    "distinct_pairs.select(\n",
    "    col(\"datasetA.processId\").alias(\"processIdA\"),\n",
    "    col(\"datasetB.processId\").alias(\"processIdB\"),\n",
    "    col(\"JaccardDistance\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9cbe7460-a4a8-437b-9105-7a6148a5d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity\n",
    "# Join to get the original features for each process\n",
    "joined_pairs = distinct_pairs \\\n",
    "    .join(sparse_vectors_df.alias(\"a\"), col(\"datasetA.processId\") == col(\"a.processId\")) \\\n",
    "    .join(sparse_vectors_df.alias(\"b\"), col(\"datasetB.processId\") == col(\"b.processId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19eef707-b62c-4559-8604-3154c4255802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+---------+--------------------+---------+--------------------+-----------------------+\n",
      "|            datasetA|            datasetB|   JaccardDistance|processId|            features|processId|            features|ActualJaccardSimilarity|\n",
      "+--------------------+--------------------+------------------+---------+--------------------+---------+--------------------+-----------------------+\n",
      "|{4, (512,[54,143,...|{5, [[1.75464546E...|               0.0|        4|(512,[54,143,204,...|        5|(512,[54,143,204,...|                    1.0|\n",
      "|{3, (512,[41,45,4...|{5, [[1.75464546E...|0.9615384615384616|        3|(512,[41,45,49,58...|        5|(512,[54,143,204,...|   0.038461538461538464|\n",
      "|{1, (512,[117,124...|{2, [[8.838168E7]...|               0.0|        1|(512,[117,124,163...|        2|(512,[117,124,163...|                    1.0|\n",
      "|{3, (512,[41,45,4...|{4, [[1.75464546E...|0.9615384615384616|        3|(512,[41,45,49,58...|        4|(512,[54,143,204,...|   0.038461538461538464|\n",
      "+--------------------+--------------------+------------------+---------+--------------------+---------+--------------------+-----------------------+\n",
      "\n",
      "+----------+----------+------------------+-----------------------+\n",
      "|processIdA|processIdB|JaccardDistance   |ActualJaccardSimilarity|\n",
      "+----------+----------+------------------+-----------------------+\n",
      "|4         |5         |0.0               |1.0                    |\n",
      "|3         |5         |0.9615384615384616|0.038461538461538464   |\n",
      "|1         |2         |0.0               |1.0                    |\n",
      "|3         |4         |0.9615384615384616|0.038461538461538464   |\n",
      "+----------+----------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "\n",
    "# Define Jaccard similarity function\n",
    "def jaccard_similarity(x, y):\n",
    "    x_set = set(x.indices)\n",
    "    y_set = set(y.indices)\n",
    "    intersection = len(x_set & y_set)\n",
    "    union = len(x_set | y_set)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Register the function as a UDF\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, DoubleType())\n",
    "\n",
    "# Add a column with the actual Jaccard similarity\n",
    "result_df = joined_pairs.withColumn(\n",
    "    \"ActualJaccardSimilarity\",\n",
    "    jaccard_similarity_udf(col(\"a.features\"), col(\"b.features\"))\n",
    ")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "# Select and show the relevant columns\n",
    "result_df.select(\n",
    "    col(\"datasetA.processId\").alias(\"processIdA\"),\n",
    "    col(\"datasetB.processId\").alias(\"processIdB\"),\n",
    "    col(\"JaccardDistance\"),\n",
    "    col(\"ActualJaccardSimilarity\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65800d66-6dfe-4e59-917c-4db918374690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, sort_array\n",
    "\n",
    "# Group similar pairs\n",
    "grouped_pairs = result_df.groupby(col(\"datasetA.processId\").alias(\"group_id\")).agg(sort_array(collect_list(col(\"datasetB.processId\"))).alias(\"similar_items\"))\n",
    "\n",
    "# Convert the grouped pairs to the desired format\n",
    "output_text = grouped_pairs.rdd.map(lambda row: f\"Group {{{row['group_id']} {','.join(map(str, row['similar_items']))}}}\").collect()\n",
    "\n",
    "# Save the output to a text file\n",
    "with open(\"similar_groups1.txt\", \"w\") as file:\n",
    "    for line in output_text:\n",
    "        file.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8ef0ba6-5e46-49c1-9f90-bad26333a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+\n",
      "|processId|signature                 |\n",
      "+---------+--------------------------+\n",
      "|1        |[0.0,19.0,12.0,43.0,19.0] |\n",
      "|6        |[21.0,2.0,54.0,33.0,6.0]  |\n",
      "|3        |[22.0,195.0,1.0,16.0,5.0] |\n",
      "|5        |[42.0,59.0,76.0,11.0,10.0]|\n",
      "|4        |[42.0,59.0,76.0,11.0,10.0]|\n",
      "|2        |[0.0,19.0,12.0,43.0,19.0] |\n",
      "+---------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Create signature matrix :\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "# Step 1: Define the number of hash functions\n",
    "num_hash_functions = 5\n",
    "\n",
    "# Step 2: Generate MinHash signatures for each process\n",
    "def generate_minhash_signature(process_features, num_buckets):\n",
    "    signature = [float('inf')] * num_hash_functions\n",
    "    for bucket in process_features:\n",
    "        for i in range(num_hash_functions):\n",
    "            hash_value = hash(str(i) + \"_\" + str(bucket)) % num_buckets\n",
    "            signature[i] = min(signature[i], hash_value)\n",
    "    return Vectors.dense(signature)\n",
    "\n",
    "# Assuming your characteristic_matrix is a DataFrame with processId and bucket columns\n",
    "# Convert characteristic_matrix to RDD and map to create signatures\n",
    "signature_rdd = characteristic_matrix.rdd.map(lambda row: (row.processId, generate_minhash_signature([i for i, val in enumerate(row[1:]) if val == 1], num_buckets)))\n",
    "signature_df = spark.createDataFrame(signature_rdd, [\"processId\", \"signature\"])\n",
    "\n",
    "# Display the signature matrix\n",
    "signature_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71ec00-6cb5-41d0-a591-6ae97e9204ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
