{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMv9VbxPxQ3ju90kFyXEbWN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15R22DJsDOox","executionInfo":{"status":"ok","timestamp":1704231953380,"user_tz":-120,"elapsed":2420,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"ef7774c8-a0cd-4341-8a86-419a7584e9f2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sVT-OdPWBHpw","executionInfo":{"status":"ok","timestamp":1704231978837,"user_tz":-120,"elapsed":25459,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"outputs":[],"source":["import h5py\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from scipy.signal import butter, filtfilt\n","import matplotlib.pyplot as plt\n","import gc\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, LSTM, Dense, Flatten, BatchNormalization, Dropout\n","from tensorflow.keras.layers import TimeDistributed, Reshape, Permute\n","from keras.utils import to_categorical\n","from tensorflow import keras\n","import pywt\n"]},{"cell_type":"code","source":["# ------------------------------ Train Data ------------------------------ #"],"metadata":{"id":"OMBG3Q3mDcGC","executionInfo":{"status":"ok","timestamp":1704231978838,"user_tz":-120,"elapsed":4,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def meg_data_generator(data_folder, task_numbers, batch_size):\n","  while True:\n","      for task in task_numbers:\n","          file_paths, labels = get_file_paths_and_labels(data_folder, task)\n","          for i in range(0, len(file_paths), batch_size):\n","              batch_paths = file_paths[i:i + batch_size]\n","              batch_labels = labels[i:i + batch_size]\n","\n","              batch_data = []\n","              for path in batch_paths:\n","                  data = load_data(path)\n","                  data = preprocess_data(data)  # scaling, filtering, downsampling.\n","                  batch_data.append(data)\n","\n","              # Reshape and concatenate data from each file in the batch\n","              batch_data = np.concatenate(batch_data, axis=0)  # Shape becomes (4*248, 8906)\n","              batch_data = np.expand_dims(batch_data, axis=-1)  # Add an extra dimension at the start\n","\n","              # Prepare labels (assumes labels are replicated for each sensor within a file)\n","              batch_labels = np.repeat(batch_labels, 248, axis=0)  # Replicate labels for each sensor\n","              batch_labels = to_categorical(batch_labels, num_classes=4)\n","\n","              print(\"\\nData appended:\", batch_data.shape)\n","              yield batch_data, batch_labels\n","\n","# Helper function to get file paths and labels\n","def get_file_paths_and_labels(data_folder, task_numbers):\n","    file_paths = []\n","    labels = []\n","    for task_number in task_numbers:\n","        for file in os.listdir(data_folder):\n","            if file.endswith(task_number + 'h5'):\n","                file_path = os.path.join(data_folder, file)\n","                file_paths.append(file_path)\n","                labels.append(assign_label(file))\n","    return file_paths, labels\n","\n","# Include your preprocessing steps here\n","def preprocess_data(data):\n","  data = data[:, :-8]\n","  data = apply_scaling(data)\n","  data = apply_lowpass(data)\n","  data = apply_downsampling(data)\n","  # print(\"data after preprocessing:\", data.shape)\n","  return data\n","\n","def load_data(file_path):\n","    with h5py.File(file_path, 'r') as f:\n","        dataset_name = get_dataset_name(file_path)\n","        matrix = f.get(dataset_name)[:]\n","        return matrix\n","\n","def get_dataset_name(file_name_with_dir):\n","    filename_without_dir = file_name_with_dir.split('/')[-1]\n","    temp = filename_without_dir.split('_')[:-1]\n","    dataset_name = \"_\".join(temp)\n","    return dataset_name\n","\n","def assign_label(file_name):\n","    if file_name.startswith(\"rest\"):\n","        # print(file_name, \"REST\")\n","        return 0\n","    elif file_name.startswith(\"task_motor\"):\n","        # print(file_name, \"MOTOR\")\n","        return 1\n","    elif file_name.startswith(\"task_story\"):\n","        # print(file_name, \"STORY\")\n","        return 2\n","    elif file_name.startswith(\"task_working\"):\n","        # print(file_name, \"WORKING\")\n","        return 3\n","    else:\n","        return None\n","\n","def count_files_with_task_numbers(data_folder, task_numbers):\n","    total_files = 0\n","    for file in os.listdir(data_folder):\n","        if any(file.endswith(task_number + 'h5') for task_number in task_numbers):\n","            total_files += 1\n","    return total_files\n","\n","\n","def find_fmri_data_folder(start_path):\n","    for root, dirs, files in os.walk(start_path):\n","        if 'meg_data' in dirs:\n","            return os.path.join(root, 'meg_data/Intra/train')\n","    raise Exception(\"meg_data folder not found. Please check the directory structure.\")\n","\n","\n","def butter_lowpass_filter(data, cutoff, fs, order=5):\n","    nyq = 0.5 * fs  # Nyquist Frequency\n","    normal_cutoff = cutoff / nyq\n","    # Get the filter coefficients\n","    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","    y = filtfilt(b, a, data)\n","    return y\n","\n","\n","\n","def apply_scaling(array):\n","  # print(\"array scaling\", array.shape)\n","  array_norm = np.zeros((array.shape[0],array.shape[1]))\n","  for i in range(array.shape[0]):\n","    means = np.mean(array[i])  # Calculate mean for each sensor\n","    stds = np.std(array[i])    # Calculate standard deviation for each sensor\n","    array_norm[i] = (array[i] - means) / stds   # Subtrack and divide\n","\n","  del array, means, stds\n","  gc.collect()\n","  return array_norm\n","\n","\n","def apply_lowpass(array):\n","  original_sampling_rate = 2034  # Original sampling rate\n","  downsampling_factor = 12\n","  new_sampling_rate = original_sampling_rate / downsampling_factor  # New sampling rate after downsampling\n","  cutoff_frequency = new_sampling_rate / 2  # Nyquist frequency\n","\n","  array_filtered = np.zeros((array.shape[0], array.shape[1]))\n","\n","  for i in range(array.shape[0]):  # Iterate over sensors\n","      array_filtered[i, :] = butter_lowpass_filter(array[i, :], cutoff_frequency, original_sampling_rate)\n","\n","  del array\n","  return array_filtered\n","\n","\n","def apply_downsampling(array):\n","  n_sensors, n_timepoints = array.shape\n","\n","  downsampling_factor = 12\n","  new_n_timepoints = n_timepoints // downsampling_factor\n","  array_downsamp = np.zeros((n_sensors, new_n_timepoints))\n","\n","  for sensor in range(n_sensors):\n","    array_downsamp[sensor,:] = array[sensor, ::downsampling_factor]\n","\n","  del array, new_n_timepoints, downsampling_factor, n_sensors, n_timepoints\n","  gc.collect()\n","  return array_downsamp\n","\n","\n","def build_model():\n","    model = Sequential()\n","\n","    # Convolutional layers\n","    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(2968,1)))\n","    # model.add(BatchNormalization())\n","    # model.add(Dropout(0.5))\n","\n","    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n","    # model.add(BatchNormalization())\n","    # model.add(Dropout(0.5))\n","\n","    # Flatten the output to feed into the LSTM layer\n","    model.add(Permute((2, 1)))\n","\n","    # LSTM layer\n","    model.add(LSTM(64, return_sequences=True))\n","    model.add(LSTM(32))\n","\n","    # Dense layers for classification\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(32, activation='relu'))\n","\n","    # Output layer\n","    model.add(Dense(4, activation='softmax')) # Assuming 4 motor tasks\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","\n","\n","fmri_data_folder = find_fmri_data_folder('/content/drive/My Drive')\n","print(\"fmri_data_folder:\", fmri_data_folder)\n","task_numbers = ['_1.','_2.','_3.','_4.','_5.','_6.','_7.','_8.']\n","batch_size = 2\n","epochs = 4\n","\n","# Initialize model here\n","model = build_model()\n","\n","# Create data generator\n","train_generator = meg_data_generator(fmri_data_folder, task_numbers, batch_size)\n","print(\"train_generator\", train_generator)\n","\n","\n","# Determine the number of steps per epoch\n","total_files = count_files_with_task_numbers(fmri_data_folder, task_numbers)\n","print(total_files)\n","steps_per_epoch = total_files // batch_size\n","\n","# Train the model\n","model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iy_nAMoABN4A","executionInfo":{"status":"ok","timestamp":1704233323544,"user_tz":-120,"elapsed":1344709,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"ec303318-b5a2-41bc-ac00-5082e7b27a2f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["fmri_data_folder: /content/drive/My Drive/Courses/Pattern Recognition/Lab/Group Assignment/meg_data/Intra/train\n","train_generator <generator object meg_data_generator at 0x7d7a011c3e60>\n","32\n","\n","Data appended: (496, 2968, 1)\n","Epoch 1/4\n","\n","Data appended: (496, 2968, 1)\n","\n","Data appended: (496, 2968, 1)\n"," 1/16 [>.............................] - ETA: 9:27 - loss: 1.3850 - accuracy: 0.0060\n","Data appended: (496, 2968, 1)\n"," 2/16 [==>...........................] - ETA: 4:37 - loss: 1.4681 - accuracy: 0.0030\n","Data appended: (496, 2968, 1)\n"," 3/16 [====>.........................] - ETA: 4:15 - loss: 1.4343 - accuracy: 0.1532\n","Data appended: (496, 2968, 1)\n"," 4/16 [======>.......................] - ETA: 4:02 - loss: 1.4308 - accuracy: 0.1149\n","Data appended: (496, 2968, 1)\n"," 5/16 [========>.....................] - ETA: 3:45 - loss: 1.4202 - accuracy: 0.0923\n","Data appended: (496, 2968, 1)\n"," 6/16 [==========>...................] - ETA: 3:22 - loss: 1.4127 - accuracy: 0.1512\n","Data appended: (496, 2968, 1)\n"," 7/16 [============>.................] - ETA: 3:01 - loss: 1.4091 - accuracy: 0.1296\n","Data appended: (496, 2968, 1)\n"," 8/16 [==============>...............] - ETA: 2:43 - loss: 1.4064 - accuracy: 0.1134\n","Data appended: (496, 2968, 1)\n"," 9/16 [===============>..............] - ETA: 2:23 - loss: 1.4050 - accuracy: 0.1564\n","Data appended: (496, 2968, 1)\n","10/16 [=================>............] - ETA: 2:02 - loss: 1.4043 - accuracy: 0.1407\n","Data appended: (496, 2968, 1)\n","11/16 [===================>..........] - ETA: 1:41 - loss: 1.4017 - accuracy: 0.1279\n","Data appended: (496, 2968, 1)\n","12/16 [=====================>........] - ETA: 1:21 - loss: 1.3987 - accuracy: 0.1358\n","Data appended: (496, 2968, 1)\n","13/16 [=======================>......] - ETA: 1:01 - loss: 1.4040 - accuracy: 0.1253\n","Data appended: (496, 2968, 1)\n","14/16 [=========================>....] - ETA: 40s - loss: 1.4052 - accuracy: 0.1164 \n","Data appended: (496, 2968, 1)\n","15/16 [===========================>..] - ETA: 20s - loss: 1.4058 - accuracy: 0.1086\n","Data appended: (496, 2968, 1)\n","16/16 [==============================] - 342s 20s/step - loss: 1.4042 - accuracy: 0.1018\n","Epoch 2/4\n","\n","Data appended: (496, 2968, 1)\n"," 1/16 [>.............................] - ETA: 4:47 - loss: 1.3747 - accuracy: 0.4940\n","Data appended: (496, 2968, 1)\n"," 2/16 [==>...........................] - ETA: 5:02 - loss: 1.3938 - accuracy: 0.2470\n","Data appended: (496, 2968, 1)\n"," 3/16 [====>.........................] - ETA: 4:32 - loss: 1.3884 - accuracy: 0.3313\n","Data appended: (496, 2968, 1)\n"," 4/16 [======>.......................] - ETA: 4:03 - loss: 1.3891 - accuracy: 0.2485\n","Data appended: (496, 2968, 1)\n"," 5/16 [========>.....................] - ETA: 3:41 - loss: 1.3870 - accuracy: 0.2988\n","Data appended: (496, 2968, 1)\n"," 6/16 [==========>...................] - ETA: 3:20 - loss: 1.3857 - accuracy: 0.3323\n","Data appended: (496, 2968, 1)\n"," 7/16 [============>.................] - ETA: 3:01 - loss: 1.3869 - accuracy: 0.2849\n","Data appended: (496, 2968, 1)\n"," 8/16 [==============>...............] - ETA: 2:42 - loss: 1.3869 - accuracy: 0.3054\n","Data appended: (496, 2968, 1)\n"," 9/16 [===============>..............] - ETA: 2:21 - loss: 1.3869 - accuracy: 0.2715\n","Data appended: (496, 2968, 1)\n","10/16 [=================>............] - ETA: 2:00 - loss: 1.3859 - accuracy: 0.3419\n","Data appended: (496, 2968, 1)\n","11/16 [===================>..........] - ETA: 1:40 - loss: 1.3859 - accuracy: 0.3563\n","Data appended: (496, 2968, 1)\n","12/16 [=====================>........] - ETA: 1:20 - loss: 1.3865 - accuracy: 0.3266\n","Data appended: (496, 2968, 1)\n","13/16 [=======================>......] - ETA: 1:04 - loss: 1.3872 - accuracy: 0.3015\n","Data appended: (496, 2968, 1)\n","14/16 [=========================>....] - ETA: 42s - loss: 1.3874 - accuracy: 0.2800 \n","Data appended: (496, 2968, 1)\n","15/16 [===========================>..] - ETA: 21s - loss: 1.3876 - accuracy: 0.2613\n","Data appended: (496, 2968, 1)\n","16/16 [==============================] - 337s 21s/step - loss: 1.3877 - accuracy: 0.2450\n","Epoch 3/4\n","\n","Data appended: (496, 2968, 1)\n"," 1/16 [>.............................] - ETA: 4:49 - loss: 1.3806 - accuracy: 0.5000\n","Data appended: (496, 2968, 1)\n"," 2/16 [==>...........................] - ETA: 4:33 - loss: 1.3860 - accuracy: 0.2500\n","Data appended: (496, 2968, 1)\n"," 3/16 [====>.........................] - ETA: 4:17 - loss: 1.3856 - accuracy: 0.3333\n","Data appended: (496, 2968, 1)\n"," 4/16 [======>.......................] - ETA: 4:06 - loss: 1.3859 - accuracy: 0.2500\n","Data appended: (496, 2968, 1)\n"," 5/16 [========>.....................] - ETA: 3:42 - loss: 1.3850 - accuracy: 0.3000\n","Data appended: (496, 2968, 1)\n"," 6/16 [==========>...................] - ETA: 3:21 - loss: 1.3844 - accuracy: 0.3333\n","Data appended: (496, 2968, 1)\n"," 7/16 [============>.................] - ETA: 3:00 - loss: 1.3860 - accuracy: 0.2857\n","Data appended: (496, 2968, 1)\n"," 8/16 [==============>...............] - ETA: 2:40 - loss: 1.3861 - accuracy: 0.3125\n","Data appended: (496, 2968, 1)\n"," 9/16 [===============>..............] - ETA: 2:21 - loss: 1.3861 - accuracy: 0.2778\n","Data appended: (496, 2968, 1)\n","10/16 [=================>............] - ETA: 2:01 - loss: 1.3853 - accuracy: 0.3500\n","Data appended: (496, 2968, 1)\n","11/16 [===================>..........] - ETA: 1:40 - loss: 1.3854 - accuracy: 0.3636\n","Data appended: (496, 2968, 1)\n","12/16 [=====================>........] - ETA: 1:20 - loss: 1.3862 - accuracy: 0.3333\n","Data appended: (496, 2968, 1)\n","13/16 [=======================>......] - ETA: 59s - loss: 1.3867 - accuracy: 0.3077 \n","Data appended: (496, 2968, 1)\n","14/16 [=========================>....] - ETA: 41s - loss: 1.3868 - accuracy: 0.2857\n","Data appended: (496, 2968, 1)\n","15/16 [===========================>..] - ETA: 20s - loss: 1.3870 - accuracy: 0.2667\n","Data appended: (496, 2968, 1)\n","16/16 [==============================] - 326s 20s/step - loss: 1.3871 - accuracy: 0.2500\n","Epoch 4/4\n","\n","Data appended: (496, 2968, 1)\n"," 1/16 [>.............................] - ETA: 4:50 - loss: 1.3805 - accuracy: 0.5000\n","Data appended: (496, 2968, 1)\n"," 2/16 [==>...........................] - ETA: 4:31 - loss: 1.3856 - accuracy: 0.2500\n","Data appended: (496, 2968, 1)\n"," 3/16 [====>.........................] - ETA: 4:22 - loss: 1.3855 - accuracy: 0.3333\n","Data appended: (496, 2968, 1)\n"," 4/16 [======>.......................] - ETA: 4:04 - loss: 1.3858 - accuracy: 0.2500\n","Data appended: (496, 2968, 1)\n"," 5/16 [========>.....................] - ETA: 3:40 - loss: 1.3848 - accuracy: 0.3000\n","Data appended: (496, 2968, 1)\n"," 6/16 [==========>...................] - ETA: 3:19 - loss: 1.3842 - accuracy: 0.3333\n","Data appended: (496, 2968, 1)\n"," 7/16 [============>.................] - ETA: 2:58 - loss: 1.3858 - accuracy: 0.2857\n","Data appended: (496, 2968, 1)\n"," 8/16 [==============>...............] - ETA: 2:38 - loss: 1.3859 - accuracy: 0.3125\n","Data appended: (496, 2968, 1)\n"," 9/16 [===============>..............] - ETA: 2:20 - loss: 1.3859 - accuracy: 0.2778\n","Data appended: (496, 2968, 1)\n","10/16 [=================>............] - ETA: 2:02 - loss: 1.3852 - accuracy: 0.3500\n","Data appended: (496, 2968, 1)\n","11/16 [===================>..........] - ETA: 1:45 - loss: 1.3853 - accuracy: 0.3636\n","Data appended: (496, 2968, 1)\n","12/16 [=====================>........] - ETA: 1:24 - loss: 1.3860 - accuracy: 0.3333\n","Data appended: (496, 2968, 1)\n","13/16 [=======================>......] - ETA: 1:02 - loss: 1.3865 - accuracy: 0.3077\n","Data appended: (496, 2968, 1)\n","14/16 [=========================>....] - ETA: 41s - loss: 1.3866 - accuracy: 0.2857 \n","Data appended: (496, 2968, 1)\n","15/16 [===========================>..] - ETA: 20s - loss: 1.3867 - accuracy: 0.2667\n","Data appended: (496, 2968, 1)\n","16/16 [==============================] - 330s 21s/step - loss: 1.3868 - accuracy: 0.2500\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7d7a04780820>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# ------------------------------ Test data ------------------------------ #"],"metadata":{"id":"QOkpRzMyBOCp","executionInfo":{"status":"ok","timestamp":1704233323544,"user_tz":-120,"elapsed":13,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Retrieve Test data\n","\n","def find_fmri_data_folder(start_path):\n","    for root, dirs, files in os.walk(start_path):\n","        if 'meg_data' in dirs:\n","            return os.path.join(root, 'meg_data/Intra/test')\n","    raise Exception(\"meg_data folder not found. Please check the directory structure.\")\n","\n","def get_dataset_name(file_name_with_dir):\n","    filename_without_dir = file_name_with_dir.split('/')[-1]\n","    temp = filename_without_dir.split('_')[:-1]\n","    dataset_name = \"_\".join(temp)\n","    return dataset_name\n","\n","def assign_label(file_name):\n","    if file_name.startswith(\"rest\"):\n","        # print(file_name, \"REST\")\n","        return 0\n","    elif file_name.startswith(\"task_motor\"):\n","        # print(file_name, \"MOTOR\")\n","        return 1\n","    elif file_name.startswith(\"task_story\"):\n","        # print(file_name, \"STORY\")\n","        return 2\n","    elif file_name.startswith(\"task_working\"):\n","        # print(file_name, \"WORKING\")\n","        return 3\n","    else:\n","        return None\n","\n","def load_data(file_path):\n","    with h5py.File(file_path, 'r') as f:\n","        dataset_name = get_dataset_name(file_path)\n","        matrix = f.get(dataset_name)[:]\n","        return matrix\n","\n","fmri_data_folder = find_fmri_data_folder('/content/drive/My Drive')\n","meg_test_data_list = []\n","labels_test = []\n","\n","for file in os.listdir(fmri_data_folder):\n","    if file.endswith('.h5'):\n","        file_path = os.path.join(fmri_data_folder, file)\n","        data = load_data(file_path)\n","        meg_test_data_list.append(data)\n","        labels_test.append(assign_label(file))\n","\n","        # Clear memory\n","        del data\n","        gc.collect()\n","\n","# Convert the list of 2D arrays into a single 3D NumPy array\n","meg_test_data_array = np.stack(meg_test_data_list, axis=0)\n","labels_test_array = np.array(labels_test)\n","\n","meg_test_data_array = meg_test_data_array[:, :, :-8]\n","\n","\n","def apply_scaling(array):\n","  array_norm = np.zeros((array.shape[0],array.shape[1],array.shape[2]))\n","  for i in range(array.shape[0]):\n","    means = np.mean(array[i], axis=1)  # Calculate mean for each sensor\n","    stds = np.std(array[i], axis=1)    # Calculate standard deviation for each sensor\n","    array_norm[i] = (array[i] - means[:, np.newaxis]) / stds[:, np.newaxis]   # Subtrack and divide\n","\n","  del array, means, stds\n","  gc.collect()\n","  return array_norm\n","\n","def butter_lowpass_filter(data, cutoff, fs, order=5):\n","    nyq = 0.5 * fs  # Nyquist Frequency\n","    normal_cutoff = cutoff / nyq\n","    # Get the filter coefficients\n","    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","    y = filtfilt(b, a, data)\n","    return y\n","\n","def apply_lowpass(array):\n","  original_sampling_rate = 2034  # Original sampling rate\n","  downsampling_factor = 12\n","  new_sampling_rate = original_sampling_rate / downsampling_factor  # New sampling rate after downsampling\n","  cutoff_frequency = new_sampling_rate / 2  # Nyquist frequency\n","\n","  array_filtered = np.zeros_like(array)\n","\n","  for o in range(array.shape[0]):\n","      for i in range(array.shape[1]):\n","          array_filtered[o, i, :] = butter_lowpass_filter(array[o, i, :], cutoff_frequency, original_sampling_rate)\n","\n","  del array\n","  return array_filtered\n","\n","\n","  del array\n","  gc.collect()\n","  return array_filtered\n","\n","\n","def apply_downsampling(array):\n","  n_observations, n_sensors, n_timepoints = array.shape\n","  downsampling_factor = 12\n","  new_n_timepoints = n_timepoints // downsampling_factor\n","\n","  array_downsamp = np.zeros((n_observations, n_sensors, new_n_timepoints))\n","\n","  for obs in range(n_observations):\n","    for sensor in range(n_sensors):\n","      array_downsamp[obs,sensor,:] = array[obs, sensor, ::downsampling_factor]\n","\n","  del array, new_n_timepoints, downsampling_factor, n_observations, n_sensors, n_timepoints\n","  gc.collect()\n","  return array_downsamp\n","\n","\n","X_task_norm = apply_scaling(meg_test_data_array)\n","del meg_test_data_array\n","gc.collect()\n","\n","# Lowpass filter\n","X_task_filtered = apply_lowpass(X_task_norm)\n","del X_task_norm\n","gc.collect()\n","\n","# Downsample\n","X_task_downsamp = apply_downsampling(X_task_filtered)\n","del X_task_filtered\n","gc.collect()\n","\n","\n","\n","obs_test, sensors_test, points_test = X_task_downsamp.shape\n","X_test = X_task_downsamp.reshape(obs_test, sensors_test, points_test)\n","y_test_encoded = to_categorical(labels_test_array, num_classes=4)"],"metadata":{"id":"c1kNrkUWnkZy","executionInfo":{"status":"ok","timestamp":1704233335806,"user_tz":-120,"elapsed":12266,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzgFaNv5pV_K","executionInfo":{"status":"ok","timestamp":1704233335806,"user_tz":-120,"elapsed":15,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"d212310c-1982-4403-f230-e6b33c782601"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 248, 2968)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["y_test_encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qnwkb8j4ps_3","executionInfo":{"status":"ok","timestamp":1704233335807,"user_tz":-120,"elapsed":13,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"7653f7cf-b332-4663-dbcc-b5f83b689cee"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0.],\n","       [0., 0., 0., 1.],\n","       [0., 0., 1., 0.],\n","       [0., 0., 1., 0.],\n","       [0., 1., 0., 0.],\n","       [1., 0., 0., 0.],\n","       [0., 0., 0., 1.],\n","       [1., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Reshaping\n","X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])  # Reshape to (8*248, 2968)\n","X_test = np.expand_dims(X_test, axis=-1)  # Add the channel dimension\n"],"metadata":{"id":"LHmS_zpgE87a","executionInfo":{"status":"ok","timestamp":1704233335807,"user_tz":-120,"elapsed":12,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["y_test_encoded = np.repeat(y_test_encoded, 248, axis=0)  # Repeat labels for each sensor\n","y_test_encoded.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXDhItrIFfr3","executionInfo":{"status":"ok","timestamp":1704233335807,"user_tz":-120,"elapsed":12,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"f3d5c9ed-50b4-4a60-bfef-bde4d0eb9337"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1984, 4)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["X_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKO9Im91FXPc","executionInfo":{"status":"ok","timestamp":1704233335807,"user_tz":-120,"elapsed":11,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"25974af0-4bd0-4423-95fb-c4f62f17c968"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1984, 2968, 1)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lln7QNQZoARq","executionInfo":{"status":"ok","timestamp":1704233378451,"user_tz":-120,"elapsed":42655,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"6a0f716d-86a7-4efd-9088-c32f9a4bf2fa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["62/62 [==============================] - 27s 412ms/step - loss: 1.3861 - accuracy: 0.2500\n"]}]},{"cell_type":"code","source":["X_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ct2SwnJXqo23","executionInfo":{"status":"ok","timestamp":1704233378452,"user_tz":-120,"elapsed":12,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"772ba8e4-654d-497c-d776-eb2dd9edbc7e"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1984, 2968, 1)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["labels_test_array"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72GYBHNxrGz8","executionInfo":{"status":"ok","timestamp":1704233378452,"user_tz":-120,"elapsed":10,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"e727eabc-b519-45dc-bf08-362e6d4f1109"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 3, 2, 2, 1, 0, 3, 0])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Make predictions for the entire test set\n","all_predictions = model.predict(X_test)\n","\n","# Convert predictions to class labels\n","predicted_labels_all = np.argmax(all_predictions, axis=1)\n","\n","# Aggregate predictions for each original test instance\n","final_predictions = []\n","for i in range(0, len(predicted_labels_all), 248):  # Iterate over each set of 248 sensors\n","    # Get the predictions for the current set of sensors\n","    sensor_predictions = predicted_labels_all[i:i + 248]\n","\n","    # Find the most frequent prediction\n","    most_frequent_prediction = np.argmax(np.bincount(sensor_predictions))\n","    final_predictions.append(most_frequent_prediction)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NV4Vk_oCe6S","executionInfo":{"status":"ok","timestamp":1704233420373,"user_tz":-120,"elapsed":41930,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"910d094a-9cf0-441e-8168-75c8dea47ef4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["62/62 [==============================] - 28s 434ms/step\n"]}]},{"cell_type":"code","source":["# Compare with actual labels\n","actual_labels = np.argmax(y_test_encoded, axis=1)  # Assuming y_test_encoded is not replicated\n"],"metadata":{"id":"fgO-EfVbHilY","executionInfo":{"status":"ok","timestamp":1704233420374,"user_tz":-120,"elapsed":16,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["final_predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhlA-tHoH5JD","executionInfo":{"status":"ok","timestamp":1704233420374,"user_tz":-120,"elapsed":15,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"4ffc4890-159c-4d42-dece-7bb01a1a6341"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 2, 2, 2, 2, 2, 2, 2]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["labels_test_array"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-C41myfH76_","executionInfo":{"status":"ok","timestamp":1704233420374,"user_tz":-120,"elapsed":13,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"fb788f76-1a84-49ab-a119-4334a6eae13c"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 3, 2, 2, 1, 0, 3, 0])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Calculate accuracy or other metrics if needed\n","correct_predictions = np.sum(np.array(final_predictions) == labels_test_array)\n","accuracy = correct_predictions / len(final_predictions)\n","print(\"Aggregated Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ha1qe3oZHhZt","executionInfo":{"status":"ok","timestamp":1704233420374,"user_tz":-120,"elapsed":11,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"7bc1861a-6a1e-4b76-8f1c-e85101a95ad8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Aggregated Accuracy: 0.25\n"]}]},{"cell_type":"code","source":["# Print individual comparisons\n","for i in range(len(labels_test_array)):\n","    print(f\"Original Instance {i}: Actual Label = {actual_labels[i]}, Aggregated Predicted Label = {final_predictions[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"doJb00h5Hedt","executionInfo":{"status":"ok","timestamp":1704233420374,"user_tz":-120,"elapsed":11,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"6572c3e8-9f2f-4746-d685-e1547aacd999"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Instance 0: Actual Label = 1, Aggregated Predicted Label = 2\n","Original Instance 1: Actual Label = 1, Aggregated Predicted Label = 2\n","Original Instance 2: Actual Label = 1, Aggregated Predicted Label = 2\n","Original Instance 3: Actual Label = 1, Aggregated Predicted Label = 2\n","Original Instance 4: Actual Label = 1, Aggregated Predicted Label = 2\n","Original Instance 5: Actual Label = 1, Aggregated Predicted Label = 2\n","Original Instance 6: Actual Label = 1, Aggregated Predicted Label = 2\n","Original Instance 7: Actual Label = 1, Aggregated Predicted Label = 2\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(labels_test_array, predicted_labels)\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n","plt.title(\"Confusion Matrix\")\n","plt.ylabel('Actual Labels')\n","plt.xlabel('Predicted Labels')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"zXV5vJsASKG1","executionInfo":{"status":"error","timestamp":1704233421371,"user_tz":-120,"elapsed":1007,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"0f28cafa-a36c-4816-ba30-1d45ca828296"},"execution_count":21,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-125d34f082d3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_test_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Plot the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'predicted_labels' is not defined"]}]},{"cell_type":"code","source":["'''\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 1s 628ms/step\n","Sample 0: Actual Label = 1, Predicted Label = 3\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 1s 638ms/step\n","Sample 1: Actual Label = 3, Predicted Label = 0\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 0s 491ms/step\n","Sample 2: Actual Label = 2, Predicted Label = 2\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 0s 480ms/step\n","Sample 3: Actual Label = 2, Predicted Label = 2\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 0s 482ms/step\n","Sample 4: Actual Label = 1, Predicted Label = 2\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 0s 492ms/step\n","Sample 5: Actual Label = 0, Predicted Label = 0\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 1s 554ms/step\n","Sample 6: Actual Label = 3, Predicted Label = 3\n","sample shape: (1, 248, 8904)\n","1/1 [==============================] - 1s 501ms/step\n","Sample 7: Actual Label = 0, Predicted Label = 0\n","'''"],"metadata":{"id":"-9h-g9-WS2Dm","executionInfo":{"status":"aborted","timestamp":1704233421371,"user_tz":-120,"elapsed":3,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":null,"outputs":[]}]}