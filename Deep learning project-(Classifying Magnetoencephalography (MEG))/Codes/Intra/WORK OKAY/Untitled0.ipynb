{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrCeP8BQGxC1hFH1pf+kl6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lG86Su1A6o5E","executionInfo":{"status":"ok","timestamp":1705218600969,"user_tz":-60,"elapsed":1782,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"e59ee177-f54e-4f2e-c82b-9f5d2c5a8ec8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import h5py\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from scipy.signal import butter, filtfilt\n","import matplotlib.pyplot as plt\n","import gc\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Activation, BatchNormalization\n","from sklearn.model_selection import GridSearchCV\n","from keras.utils import to_categorical\n","from tensorflow import keras\n","import pywt\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import Conv2D, ELU, ZeroPadding2D, MaxPooling2D\n","from tensorflow.keras import regularizers\n"],"metadata":{"id":"7BFpbyV26svT","executionInfo":{"status":"ok","timestamp":1705218611417,"user_tz":-60,"elapsed":10450,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def load_data(file_path):\n","    with h5py.File(file_path, 'r') as f:\n","        dataset_name = get_dataset_name(file_path)\n","        matrix = f.get(dataset_name)[:]\n","        return matrix\n","\n","def get_dataset_name(file_name_with_dir):\n","    filename_without_dir = file_name_with_dir.split('/')[-1]\n","    temp = filename_without_dir.split('_')[:-1]\n","    dataset_name = \"_\".join(temp)\n","    return dataset_name\n","\n","def load_data_by_task(data_folder):\n","    meg_data_list = []\n","    labels = []\n","\n","    for file in os.listdir(data_folder):\n","        if file.endswith('.h5'):\n","            file_path = os.path.join(data_folder, file)\n","            data = load_data(file_path)\n","            if data is not None:\n","                meg_data_list.append(data)\n","                label = assign_label(file)\n","                labels.append(label)\n","\n","    if meg_data_list:\n","        meg_data_array = np.stack(meg_data_list, axis=0)\n","        labels_array = np.array(labels)\n","        return meg_data_array, labels_array\n","    else:\n","        return None, None\n","\n","def assign_label(file_name):\n","    if file_name.startswith(\"rest\"):\n","        return 0\n","    elif file_name.startswith(\"task_motor\"):\n","        return 1\n","    elif file_name.startswith(\"task_story\"):\n","        return 2\n","    elif file_name.startswith(\"task_working\"):\n","        return 3\n","    else:\n","        return None\n","\n","def find_fmri_data_folder(start_path):\n","    for root, dirs, files in os.walk(start_path):\n","        if 'meg_data' in dirs:\n","            return os.path.join(root, 'meg_data/Intra/train')\n","    raise Exception(\"meg_data folder not found. Please check the directory structure.\")\n","\n","\n","def butter_lowpass_filter(data, cutoff, fs, order=5):\n","    nyq = 0.5 * fs  # Nyquist Frequency\n","    normal_cutoff = cutoff / nyq\n","    # Get the filter coefficients\n","    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","    y = filtfilt(b, a, data)\n","    return y\n","\n","\n","\n","def apply_scaling(array):\n","  array_norm = np.zeros((array.shape[0],array.shape[1],array.shape[2]))\n","  for i in range(array.shape[0]):\n","    means = np.mean(array[i], axis=1)  # Calculate mean for each sensor\n","    stds = np.std(array[i], axis=1)    # Calculate standard deviation for each sensor\n","    array_norm[i] = (array[i] - means[:, np.newaxis]) / stds[:, np.newaxis]   # Subtrack and divide\n","\n","  del array, means, stds\n","  gc.collect()\n","  return array_norm\n","\n","\n","def apply_lowpass(array):\n","  original_sampling_rate = 2034  # Original sampling rate\n","  downsampling_factor = 8\n","  new_sampling_rate = original_sampling_rate / downsampling_factor  # New sampling rate after downsampling\n","  cutoff_frequency = new_sampling_rate / 2  # Nyquist frequency\n","\n","  array_filtered = np.zeros_like(array)\n","\n","  for o in range(array.shape[0]):\n","      for i in range(array.shape[1]):\n","          array_filtered[o, i, :] = butter_lowpass_filter(array[o, i, :], cutoff_frequency, original_sampling_rate)\n","\n","  del array\n","  return array_filtered\n","\n","\n","def apply_downsampling(array):\n","  n_observations, n_sensors, n_timepoints = array.shape\n","  new_n_timepoints = n_timepoints // 8\n","  downsampling_factor = 8\n","  array_downsamp = np.zeros((n_observations, n_sensors, new_n_timepoints))\n","\n","  for obs in range(n_observations):\n","    for sensor in range(n_sensors):\n","      array_downsamp[obs,sensor,:] = array[obs, sensor, ::downsampling_factor]\n","\n","  del array, new_n_timepoints, downsampling_factor, n_observations, n_sensors, n_timepoints\n","  gc.collect()\n","  return array_downsamp\n","\n","\n","\n","\n","def apply_wavelet_transform(data, wavelet='db4', level=5, original_length=8906):\n","    transformed_data = np.zeros(data.shape)  # Initialize array to maintain original shape\n","\n","    for i in range(data.shape[0]):  # Loop over observations\n","        for j in range(data.shape[1]):  # Loop over sensors\n","            # Apply wavelet transform to the sensor's data\n","            coeffs = pywt.wavedec(data[i, j, :], wavelet, level=level)\n","            # Concatenate and pad the coefficients to match original length\n","            concatenated_coeffs = np.concatenate(coeffs)\n","            pad_length = original_length - concatenated_coeffs.shape[0]\n","            if pad_length > 0:\n","                concatenated_coeffs = np.pad(concatenated_coeffs, (0, pad_length), 'constant')\n","            else:\n","                concatenated_coeffs = concatenated_coeffs[:original_length]\n","            # Assign the transformed data\n","            transformed_data[i, j, :] = concatenated_coeffs\n","\n","    return transformed_data\n","\n","\n","fmri_data_folder = find_fmri_data_folder('/content/drive/My Drive')\n","\n","tasks = ['rest', 'task_motor', 'task_story', 'task_working']\n","task_numbers = ['_1.','_2.','_3.','_4.','_5.','_6.','_7.','_8.']\n","visual_data = []\n","\n","X_task, y_task = load_data_by_task(fmri_data_folder)\n","\n"],"metadata":{"id":"DBR6oy796sxp","executionInfo":{"status":"ok","timestamp":1705218626492,"user_tz":-60,"elapsed":15077,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if X_task is not None and y_task is not None:\n","    print(\"Train shape:\", X_task.shape)\n","    print(\"Labels shape:\", y_task.shape)\n","\n","    ##########\n","    visual_data.append(X_task[0,0])\n","\n","    # Scaling\n","    X_task_norm = apply_scaling(X_task)\n","    print(\"X_task_norm\", X_task_norm.shape)\n","    del X_task\n","    gc.collect()\n","    ##########\n","    visual_data.append(X_task_norm[0,0])\n","\n","    # Lowpass filter\n","    X_task_filtered = apply_lowpass(X_task_norm)\n","    print(\"X_task_filtered\", X_task_filtered.shape)\n","    del X_task_norm\n","    gc.collect()\n","    ##########\n","    visual_data.append(X_task_filtered[0,0])\n","\n","    # Downsample\n","    X_task_downsamp = apply_downsampling(X_task_filtered)\n","    print(\"X_task_downsamp\", X_task_downsamp.shape)\n","    del X_task_filtered\n","    gc.collect()\n","    ##########\n","    visual_data.append(X_task_downsamp[0,0])\n","\n","    print(\"After downsampling:\", X_task_downsamp.shape)\n","\n","    # ----- Train the model on this task's data ----- #\n","    obs_train, sensors_train, points_train = X_task_downsamp.shape\n","\n","    X_train = np.expand_dims(X_task_downsamp, axis=3)\n","    print(\"X_train shape:\", X_train.shape)\n","\n","    y_train_encoded = to_categorical(y_task, num_classes=4)\n","\n","    del y_task, obs_train, visual_data\n","    gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wtga6-2y6sz2","executionInfo":{"status":"ok","timestamp":1705218649564,"user_tz":-60,"elapsed":23074,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"4d900e1a-1333-4306-be2d-796b734304fd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (32, 248, 35624)\n","Labels shape: (32,)\n","X_task_norm (32, 248, 35624)\n","X_task_filtered (32, 248, 35624)\n","X_task_downsamp (32, 248, 4453)\n","After downsampling: (32, 248, 4453)\n","X_train shape: (32, 248, 4453, 1)\n"]}]},{"cell_type":"code","source":["def EEGNet():\n","    model = Sequential()\n","\n","    # Layer 1\n","    model.add(Conv2D(8, (1, 64), input_shape=(248, 4453,1), padding='valid'))\n","    model.add(BatchNormalization(axis=1))\n","    model.add(ELU())\n","    model.add(Dropout(0.25))\n","    model.add(MaxPooling2D(pool_size=(1, 4)))\n","    # No permute layer in Keras; adjust as needed\n","\n","    # Layer 2\n","    model.add(ZeroPadding2D(padding=((0, 1), (16, 17))))  # Adjusted padding\n","    model.add(Conv2D(4, (2, 32), padding='valid'))\n","    model.add(BatchNormalization(axis=1))\n","    model.add(ELU())\n","    model.add(Dropout(0.25))\n","    model.add(MaxPooling2D(pool_size=(2, 4)))\n","\n","    # Layer 3\n","    model.add(ZeroPadding2D(padding=((4, 3), (2, 1))))  # Adjusted padding\n","    model.add(Conv2D(4, (8, 4), padding='valid'))\n","    model.add(BatchNormalization(axis=1))\n","    model.add(ELU())\n","    model.add(Dropout(0.25))\n","    model.add(MaxPooling2D(pool_size=(2, 4)))\n","\n","    # FC Layer\n","    model.add(Flatten())\n","    model.add(Dense(4, activation='softmax', kernel_regularizer=regularizers.l2(0.1)))  # Output layer for 4 classes\n","\n","    return model"],"metadata":{"id":"LC1gco2x6s2S","executionInfo":{"status":"ok","timestamp":1705218666368,"user_tz":-60,"elapsed":227,"user":{"displayName":"Christos","userId":"17488364610072087916"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Create the model\n","net = EEGNet()\n","\n","# Compile the model\n","net.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","history = net.fit(X_train,y_train_encoded , epochs=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTVXcRPA6s66","executionInfo":{"status":"ok","timestamp":1705219414377,"user_tz":-60,"elapsed":745809,"user":{"displayName":"Christos","userId":"17488364610072087916"}},"outputId":"1c4ed06a-f404-4ded-d88c-0b134e95e3fc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 [==============================] - 74s 74s/step - loss: 2.5146 - accuracy: 0.2812\n","Epoch 2/10\n","1/1 [==============================] - 70s 70s/step - loss: 12.1810 - accuracy: 0.3438\n","Epoch 3/10\n","1/1 [==============================] - 69s 69s/step - loss: 5.1133 - accuracy: 0.4062\n","Epoch 4/10\n","1/1 [==============================] - 69s 69s/step - loss: 8.2208 - accuracy: 0.2500\n","Epoch 5/10\n","1/1 [==============================] - 72s 72s/step - loss: 6.4397 - accuracy: 0.2500\n","Epoch 6/10\n","1/1 [==============================] - 70s 70s/step - loss: 9.0493 - accuracy: 0.2500\n","Epoch 7/10\n","1/1 [==============================] - 70s 70s/step - loss: 6.0159 - accuracy: 0.2812\n","Epoch 8/10\n","1/1 [==============================] - 80s 80s/step - loss: 4.3530 - accuracy: 0.2500\n","Epoch 9/10\n","1/1 [==============================] - 70s 70s/step - loss: 2.7815 - accuracy: 0.4688\n","Epoch 10/10\n","1/1 [==============================] - 69s 69s/step - loss: 4.2437 - accuracy: 0.5000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"B_L3zvX76s9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U-AmgarF6s_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lMxmRACn6tBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5K4vJm896tD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CW3x0viN6tGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vqSBq-H76tJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7pLGA6eK6tMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2L8jtmz-6tPM"},"execution_count":null,"outputs":[]}]}